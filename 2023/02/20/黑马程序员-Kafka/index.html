<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>黑马程序员-Kafka | WeiBlog</title><meta name="author" content="Wei Shao"><meta name="copyright" content="Wei Shao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="简介消息队列简介什么是消息队列消息队列，英文名：Message Queue，经常缩写为MQ。从字面上来理解，消息队列是一种用来存储消息的队列。来看一下下面的代码： 123456789&#x2F;&#x2F; 1. 创建一个保存字符串的队列Queue&lt;String&gt; stringQueue &#x3D; new LinkedList&lt;String&gt;();&#x2F;&#x2F; 2. 往消息队列中放入消息stringQueu">
<meta property="og:type" content="article">
<meta property="og:title" content="黑马程序员-Kafka">
<meta property="og:url" content="https://weishao-996.github.io/2023/02/20/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/index.html">
<meta property="og:site_name" content="WeiBlog">
<meta property="og:description" content="简介消息队列简介什么是消息队列消息队列，英文名：Message Queue，经常缩写为MQ。从字面上来理解，消息队列是一种用来存储消息的队列。来看一下下面的代码： 123456789&#x2F;&#x2F; 1. 创建一个保存字符串的队列Queue&lt;String&gt; stringQueue &#x3D; new LinkedList&lt;String&gt;();&#x2F;&#x2F; 2. 往消息队列中放入消息stringQueu">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://weishao-996.github.io/img/bg/WechatIMG48.png">
<meta property="article:published_time" content="2023-02-20T12:40:50.000Z">
<meta property="article:modified_time" content="2023-03-07T12:54:26.678Z">
<meta property="article:author" content="Wei Shao">
<meta property="article:tag" content="黑马程序员">
<meta property="article:tag" content="Kafka">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://weishao-996.github.io/img/bg/WechatIMG48.png"><link rel="shortcut icon" href="/img/bg/%E6%89%8B%E7%BB%98%E7%81%AB%E7%AE%AD.png"><link rel="canonical" href="https://weishao-996.github.io/2023/02/20/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '黑马程序员-Kafka',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-07 20:54:26'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = url => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      link.onload = () => resolve()
      link.onerror = () => reject()
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/bg/WechatIMG48.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">32</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">23</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/bg/WechatIMG88.png')"><nav id="nav"><span id="blog-info"><a href="/" title="WeiBlog"><span class="site-name">WeiBlog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">黑马程序员-Kafka</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-02-20T12:40:50.000Z" title="发表于 2023-02-20 20:40:50">2023-02-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-03-07T12:54:26.678Z" title="更新于 2023-03-07 20:54:26">2023-03-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/">消息中间件</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/Kafka/">Kafka</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="黑马程序员-Kafka"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="消息队列简介"><a href="#消息队列简介" class="headerlink" title="消息队列简介"></a>消息队列简介</h3><h4 id="什么是消息队列"><a href="#什么是消息队列" class="headerlink" title="什么是消息队列"></a>什么是消息队列</h4><p>消息队列，英文名：Message Queue，经常缩写为MQ。从字面上来理解，消息队列是一种用来存储消息的队列。来看一下下面的代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 创建一个保存字符串的队列</span></span><br><span class="line">Queue&lt;String&gt; stringQueue = <span class="keyword">new</span> <span class="title class_">LinkedList</span>&lt;String&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 往消息队列中放入消息</span></span><br><span class="line">stringQueue.offer(<span class="string">&quot;hello&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 从消息队列中取出消息并打印</span></span><br><span class="line">System.out.println(stringQueue.poll());</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>上述代码，创建了一个队列，先往队列中添加了一个消息，然后又从队列中取出了一个消息。这说明了队列是可以用来存取消息的。我们可以简单理解消息队列就是<strong>将需要传输的数据存放在队列中</strong>。</p>
<h4 id="消息队列中间件"><a href="#消息队列中间件" class="headerlink" title="消息队列中间件"></a>消息队列中间件</h4><p>消息队列中间件就是用来存储消息的软件（组件）。举个例子来理解，为了分析网站的用户行为，我们需要记录用户的访问日志。这些一条条的日志，可以看成是一条条的消息，我们可以将它们保存到消息队列中。将来有一些应用程序需要处理这些日志，就可以随时将这些消息取出来处理。</p>
<p>目前市面上的消息队列有很多，例如：Kafka、RabbitMQ、ActiveMQ、RocketMQ、ZeroMQ等。</p>
<h5 id="为什么叫Kafka呢"><a href="#为什么叫Kafka呢" class="headerlink" title="为什么叫Kafka呢"></a>为什么叫Kafka呢</h5><p>Kafka的架构师jay kreps非常喜欢franz kafka（弗兰兹·卡夫卡）,并且觉得kafka这个名字很酷，因此取了个和消息传递系统完全不相干的名称kafka，该名字并没有特别的含义。</p>
<p>「也就是说，你特别喜欢尼古拉斯赵四，将来你做一个项目，也可以把项目的名字取名为：尼古拉斯赵四，然后这个项目就火了」</p>
<h4 id="消息队列的应用场景"><a href="#消息队列的应用场景" class="headerlink" title="消息队列的应用场景"></a>消息队列的应用场景</h4><h5 id="异步处理"><a href="#异步处理" class="headerlink" title="异步处理"></a>异步处理</h5><p>电商网站中，新的用户注册时，需要将用户的信息保存到数据库中，同时还需要额外发送注册的邮件通知、以及短信注册码给用户。但因为发送邮件、发送注册短信需要连接外部的服务器，需要额外等待一段时间，此时，就可以使用消息队列来进行异步处理，从而实现快速响应。</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230220204957771.png" alt="image-20230220204957771"></p>
<h5 id="系统解耦"><a href="#系统解耦" class="headerlink" title="系统解耦"></a>系统解耦</h5><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230220205100592.png" alt="image-20230220205100592"></p>
<h5 id="流量削峰"><a href="#流量削峰" class="headerlink" title="流量削峰"></a>流量削峰</h5><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230220205155405.png" alt="image-20230220205155405"></p>
<h5 id="日志处理（大数据领域常见）"><a href="#日志处理（大数据领域常见）" class="headerlink" title="日志处理（大数据领域常见）"></a>日志处理（大数据领域常见）</h5><p>大型电商网站（淘宝、京东、国美、苏宁…）、App（抖音、美团、滴滴等）等需要分析用户行为，要根据用户的访问行为来发现用户的喜好以及活跃情况，需要在页面上收集大量的用户访问信息。</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230220205338436.png" alt="image-20230220205338436"></p>
<h4 id="生产者、消费者模型"><a href="#生产者、消费者模型" class="headerlink" title="生产者、消费者模型"></a>生产者、消费者模型</h4><p>我们之前学习过Java的服务器开发，Java服务器端开发的交互模型是这样的：</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230220205417595.png" alt="image-20230220205417595"></p>
<p>我们之前也学习过使用Java JDBC来访问操作MySQL数据库，它的交互模型是这样的：</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230220205444286.png" alt="image-20230220205444286"></p>
<p>它也是一种请求响应模型，只不过它不再是基于http协议，而是基于MySQL数据库的通信协议。</p>
<p>而如果我们基于消息队列来编程，此时的交互模式成为：生产者、消费者模型。</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230220205531093.png" alt="image-20230220205531093"></p>
<h4 id="消息队列的两种模式"><a href="#消息队列的两种模式" class="headerlink" title="消息队列的两种模式"></a>消息队列的两种模式</h4><h5 id="点对点模式"><a href="#点对点模式" class="headerlink" title="点对点模式"></a>点对点模式</h5><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230220205611571.png" alt="image-20230220205611571"></p>
<p>消息发送者生产消息发送到消息队列中，然后消息接收者从消息队列中取出并且消费消息。消息被消费以后，消息队列中不再有存储，所以消息接收者不可能消费到已经被消费的消息。</p>
<p>点对点模式特点：</p>
<ul>
<li>每个消息只有一个接收者（Consumer）(即一旦被消费，消息就不再在消息队列中)</li>
<li>发送者和接收者间没有依赖性，发送者发送消息之后，不管有没有接收者在运行，都不会影响到发送者下次发送消息；</li>
<li>接收者在成功接收消息之后需向队列应答成功，以便消息队列删除当前接收的消息；</li>
</ul>
<h5 id="发布订阅模式"><a href="#发布订阅模式" class="headerlink" title="发布订阅模式"></a>发布订阅模式</h5><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230220205821230.png" alt="image-20230220205821230"></p>
<p>发布/订阅模式特点：</p>
<ul>
<li>每个消息可以有多个订阅者；</li>
<li>发布者和订阅者之间有时间上的依赖性。针对某个主题（Topic）的订阅者，它必须创建一个订阅者之后，才能消费发布者的消息。</li>
<li>为了消费消息，订阅者需要提前订阅该角色主题，并保持在线运行；</li>
</ul>
<h3 id="Kafka简介"><a href="#Kafka简介" class="headerlink" title="Kafka简介"></a>Kafka简介</h3><h4 id="什么是Kafka"><a href="#什么是Kafka" class="headerlink" title="什么是Kafka"></a>什么是Kafka</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230220205954332.png" alt="image-20230220205954332"></p>
<p>Kafka是由Apache软件基金会开发的一个开源流平台，由Scala和Java编写。Kafka的Apache官网是这样介绍Kakfa的。</p>
<p>  Apache Kafka是一个分布式流平台。一个分布式的流平台应该包含3点关键的能力：  </p>
<ol>
<li>发布和订阅流数据流，类似于消息队列或者是企业消息传递系统  </li>
<li>以容错的持久化方式存储数据流  </li>
<li>处理数据流  </li>
</ol>
<h4 id="Kafka的应用场景"><a href="#Kafka的应用场景" class="headerlink" title="Kafka的应用场景"></a>Kafka的应用场景</h4><p>我们通常将Apache Kafka用在两类程序：</p>
<ol>
<li><p>建立实时数据管道，以可靠地在系统或应用程序之间获取数据</p>
</li>
<li><p>构建实时流应用程序，以转换或响应数据流</p>
</li>
</ol>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230220210211153.png" alt="image-20230220210211153"></p>
<p>上图，我们可以看到：</p>
<ol>
<li><p><code>Producers</code>：可以有很多的应用程序，将消息数据放入到Kafka集群中。</p>
</li>
<li><p><code>Consumers</code>：可以有很多的应用程序，将消息数据从Kafka集群中拉取出来。</p>
</li>
<li><p><code>Connectors</code>：Kafka的连接器可以将数据库中的数据导入到Kafka，也可以将Kafka的数据导出到</p>
</li>
</ol>
<p>数据库中。</p>
<ol start="4">
<li><code>Stream Processors</code>：流处理器可以Kafka中拉取数据，也可以将数据写入到Kafka中。</li>
</ol>
<h4 id="Kafka诞生背景"><a href="#Kafka诞生背景" class="headerlink" title="Kafka诞生背景"></a>Kafka诞生背景</h4><p>kafka的诞生，是为了解决linkedin的数据管道问题，起初linkedin采用了ActiveMQ来进行数据交换，大约是在2010年前后，那时的ActiveMQ还远远无法满足linkedin对数据传递系统的要求，经常由于各种缺陷而导致消息阻塞或者服务无法正常访问，为了能够解决这个问题，linkedin决定研发自己的消息传递系统，当时linkedin的首席架构师jay kreps便开始组织团队进行消息传递系统的研发。</p>
<p>提示：</p>
<ol>
<li>Linkedin还是挺牛逼的  </li>
<li> Kafka比ActiveMQ牛逼得多  </li>
</ol>
<h3 id="Kafka的优势"><a href="#Kafka的优势" class="headerlink" title="Kafka的优势"></a>Kafka的优势</h3><p>前面我们了解到，消息队列中间件有很多，为什么我们要选择Kafka？</p>
<table>
<thead>
<tr>
<th>特性</th>
<th>ActiveMQ</th>
<th>RabbitMQ</th>
<th>Kafka</th>
<th>RocketMQ</th>
</tr>
</thead>
<tbody><tr>
<td>所属社区/公司</td>
<td>Apache</td>
<td>Mozilla Public License</td>
<td>Apache</td>
<td>Apache/Ali</td>
</tr>
<tr>
<td>成熟度</td>
<td>成熟</td>
<td>成熟</td>
<td>成熟</td>
<td>比较成熟</td>
</tr>
<tr>
<td>生产者-消费者模式</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>发布-订阅</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>REQUEST-REPLY</td>
<td>支持</td>
<td>支持</td>
<td>-</td>
<td>支持</td>
</tr>
<tr>
<td>API完备性</td>
<td>高</td>
<td>高</td>
<td>高</td>
<td>低（静态配置）</td>
</tr>
<tr>
<td>多语言支持</td>
<td>支持JAVA优先</td>
<td>语言无关</td>
<td>支持，JAVA优先</td>
<td>支持</td>
</tr>
<tr>
<td>单机呑吐量</td>
<td>万级（最差）</td>
<td>万级</td>
<td><strong>十万级</strong></td>
<td>十万级（最高）</td>
</tr>
<tr>
<td>消息延迟</td>
<td>-</td>
<td>微秒级</td>
<td><strong>毫秒级</strong></td>
<td>-</td>
</tr>
<tr>
<td>可用性</td>
<td>高（主从）</td>
<td>高（主从）</td>
<td><strong>非常高（分布式）</strong></td>
<td>高</td>
</tr>
<tr>
<td>消息丢失</td>
<td>-</td>
<td>低</td>
<td><strong>理论上不会丢失</strong></td>
<td>-</td>
</tr>
<tr>
<td>消息重复</td>
<td>-</td>
<td>可控制</td>
<td>理论上会有重复</td>
<td>-</td>
</tr>
<tr>
<td>事务</td>
<td>支持</td>
<td>不支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>文档的完备性</td>
<td>高</td>
<td>高</td>
<td>高</td>
<td>中</td>
</tr>
<tr>
<td>提供快速入门</td>
<td>有</td>
<td>有</td>
<td>有</td>
<td>无</td>
</tr>
<tr>
<td>首次部署难度</td>
<td>-</td>
<td>低</td>
<td>中</td>
<td>高</td>
</tr>
</tbody></table>
<p>在大数据技术领域，一些重要的组件、框架都支持Apache Kafka，不论成成熟度、社区、性能、可靠性，Kafka都是非常有竞争力的一款产品。</p>
<h3 id="哪些公司在使用Kafka"><a href="#哪些公司在使用Kafka" class="headerlink" title="哪些公司在使用Kafka"></a>哪些公司在使用Kafka</h3><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230220210752801.png" alt="image-20230220210752801"></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230220211406169.png" alt="image-20230220211406169"></p>
<h3 id="Kafka生态圈介绍"><a href="#Kafka生态圈介绍" class="headerlink" title="Kafka生态圈介绍"></a>Kafka生态圈介绍</h3><p>Apache Kafka这么多年的发展，目前也有一个较庞大的生态圈。</p>
<p>Kafka生态圈官网地址：<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem">https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem</a></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230220211450569.png" alt="image-20230220211450569"></p>
<h3 id="Kafka版本"><a href="#Kafka版本" class="headerlink" title="Kafka版本"></a>Kafka版本</h3><p>本次课程使用的Kafka版本为2.4.1，是2020年3月12日发布的版本。</p>
<p>可以注意到Kafka的版本号为：kafka_2.12-2.4.1，因为kafka主要是使用scala语言开发的，2.12为scala的版本号。<a target="_blank" rel="noopener" href="http://kafka.apache.org/downloads%E5%8F%AF%E4%BB%A5%E6%9F%A5%E7%9C%8B%E5%88%B0%E6%AF%8F%E4%B8%AA%E7%89%88%E6%9C%AC%E7%9A%84%E5%8F%91%E5%B8%83%E6%97%B6%E9%97%B4%E3%80%82">http://kafka.apache.org/downloads可以查看到每个版本的发布时间。</a></p>
<h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><h3 id="搭建Kafka集群"><a href="#搭建Kafka集群" class="headerlink" title="搭建Kafka集群"></a>搭建Kafka集群</h3><p>以下基于ubuntu22.04.1</p>
<ol>
<li>将Kafka的安装包上传到虚拟机，并解压</li>
</ol>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230228164225219.png" alt="image-20230228164225219"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir export</span><br><span class="line">cd /export</span><br><span class="line">sudo mkdir server</span><br><span class="line">sudo mkdir software</span><br><span class="line">sudo chmod 777 software/</span><br><span class="line">sudo chmod 777 server/</span><br><span class="line">cd /export/software/</span><br><span class="line">tar -xvzf kafka_2.12-2.4.1.tgz -C ../server/</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>修改 server.properties</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建Kafka数据的位置</span></span><br><span class="line">mkdir /export/server/kafka_2.12-2.4.1/data</span><br><span class="line">vim /export/server/kafka_2.12-2.4.1/config/server.properties</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定broker的<span class="built_in">id</span></span></span><br><span class="line">broker.id=0</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定Kafka数据的位置</span></span><br><span class="line">log.dirs=/export/server/kafka_2.12-2.4.1/data</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置zk的三个节点</span></span><br><span class="line">zookeeper.connect=10.211.55.8:2181,10.211.55.9:2181,10.211.55.7:2181</span><br></pre></td></tr></table></figure>

<p>其余两台服务器重复以上步骤,仅修改<code>broker.id</code></p>
<ol start="3">
<li>配置KAFKA_HOME环境变量</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sudo su</span><br><span class="line">vim /etc/profile</span><br><span class="line">export KAFKA_HOME=/export/server/kafka_2.12-2.4.1</span><br><span class="line">export PATH=:$PATH:$&#123;KAFKA_HOME&#125;</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">源文件无下面这条需手动添加</span></span><br><span class="line">export PATH</span><br><span class="line"></span><br><span class="line">每个节点加载环境变量</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>启动服务器</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动ZooKeeper 见黑马zookeeper集群搭建</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动Kafka,需要在kafka根目录下启动</span></span><br><span class="line">cd /export/server/kafka_2.12-2.4.1</span><br><span class="line">nohup bin/kafka-server-start.sh config/server.properties &amp;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">测试Kafka集群是否启动成功</span></span><br><span class="line">bin/kafka-topics.sh --bootstrap-server 10.211.55.8:9092 --list</span><br></pre></td></tr></table></figure>

<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230301160650636.png" alt="image-20230301160650636"></p>
<p>无任何报错即成功。</p>
<h3 id="目录结构分析"><a href="#目录结构分析" class="headerlink" title="目录结构分析"></a>目录结构分析</h3><table>
<thead>
<tr>
<th>目录名称</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>bin</td>
<td>Kafka的所有执行脚本都在这里。例如：启动Kafka服务器、创建Topic、生产者、消费者程序等等</td>
</tr>
<tr>
<td>config</td>
<td>Kafka的所有配置文件</td>
</tr>
<tr>
<td>libs</td>
<td>运行Kafka所需要的所有JAR包</td>
</tr>
<tr>
<td>logs</td>
<td>Kafka的所有日志文件，如果Kafka出现一些问题，需要到该目录中去查看异常信息</td>
</tr>
<tr>
<td>site-docs</td>
<td>Kafka的网站帮助文件</td>
</tr>
</tbody></table>
<h3 id="Kafka一键启动-关闭脚本"><a href="#Kafka一键启动-关闭脚本" class="headerlink" title="Kafka一键启动/关闭脚本"></a>Kafka一键启动/关闭脚本</h3><p>为了方便将来进行一键启动、关闭Kafka，我们可以编写一个shell脚本来操作。将来只要执行一次该脚本就可以快速启动/关闭Kafka。</p>
<ol>
<li>在节点1中创建 /export/onekey 目录</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir onekey</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>准备slave配置文件，用于保存要启动哪几个节点上的<strong>kafka</strong></li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd /export/onekey</span><br><span class="line">sudo su</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">新建slave文件</span></span><br><span class="line">touch slave</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">slave中写入以下内容</span></span><br><span class="line">10.211.55.8</span><br><span class="line">10.211.55.9</span><br><span class="line">10.211.55.7</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>编写start-kafka.sh脚本</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vim start-kafka.sh</span><br><span class="line"></span><br><span class="line">cat /export/onekey/slave | while read line</span><br><span class="line">do</span><br><span class="line">&#123;</span><br><span class="line"> echo $line</span><br><span class="line"> ssh $line &quot;source /etc/profile;export JMX_PORT=9988;nohup $&#123;KAFKA_HOME&#125;/bin/kafka-server-start.sh $&#123;KAFKA_HOME&#125;/config/server.properties &gt;/dev/nul* 2&gt;&amp;1 &amp; &quot;</span><br><span class="line"> wait</span><br><span class="line">&#125;&amp;</span><br><span class="line">done</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol start="4">
<li>编写stop-kafka.sh脚本</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vim stop-kafka.sh</span><br><span class="line"></span><br><span class="line">cat /export/onekey/slave | while read line</span><br><span class="line">do</span><br><span class="line">&#123;</span><br><span class="line"> echo $line</span><br><span class="line"> ssh $line &quot;source /etc/profile;jps |grep Kafka |cut -d&#x27; &#x27; -f1 |xargs kill -s 9&quot;</span><br><span class="line"> wait</span><br><span class="line">&#125;&amp;</span><br><span class="line">done</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol start="5">
<li>给start-kafka.sh、stop-kafka.sh配置执行权限</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod u+x start-kafka.sh</span><br><span class="line">chmod u+x stop-kafka.sh</span><br></pre></td></tr></table></figure>

<ol start="6">
<li><p>执行一键启动、一键关闭</p>
<p>执行shell脚本需实现服务期间ssh免密登录</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u010044182/article/details/128664248">(69条消息) Ubuntu开启SSH免密登录_ubuntu配置ssh免密登录_天雪浪子的博客-CSDN博客</a></p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./start-kafka.sh</span><br><span class="line">./stop-kafka.sh</span><br></pre></td></tr></table></figure>

<p>当查看日志发生<code>Error connecting to node ubuntu2:9092</code>错误时需在三台服务器上配置如下命令</p>
<p>以ubuntu2为例,另外两台同样的规则配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/hosts</span><br><span class="line"> </span><br><span class="line">10.211.55.8 ubuntu1</span><br><span class="line">10.211.55.7 ubuntu3</span><br></pre></td></tr></table></figure>

<h2 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h2><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230302165113380.png" alt="image-20230302165113380"></p>
<h3 id="创建topic"><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h3><p>创建一个<code>topic</code>（主题）。Kafka中所有的消息都是保存在主题中，要生产消息到Kafka，首先必须要有一个确定的主题。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建名为<span class="built_in">test</span>的主题</span></span><br><span class="line">bin/kafka-topics.sh --create --bootstrap-server 10.211.55.8:9092 --topic test</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看目前Kafka中的主题</span></span><br><span class="line">bin/kafka-topics.sh --list --bootstrap-server 10.211.55.8:9092</span><br></pre></td></tr></table></figure>

<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230302165400395.png" alt="image-20230302165400395"></p>
<h3 id="生产消息到Kafka"><a href="#生产消息到Kafka" class="headerlink" title="生产消息到Kafka"></a>生产消息到Kafka</h3><p>使用Kafka内置的测试程序，生产一些消息到Kafka的<code>test</code>主题中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list 10.211.55.8:9092 --topic test</span><br></pre></td></tr></table></figure>

<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230302165621183.png" alt="image-20230302165621183"></p>
<p>“&gt;”表示等待输入 </p>
<h3 id="从Kafka消费消息"><a href="#从Kafka消费消息" class="headerlink" title="从Kafka消费消息"></a>从Kafka消费消息</h3><p>使用下面的命令来消费 <code>test </code>主题中的消息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server 10.211.55.8:9092 --topic test --from-beginning</span><br></pre></td></tr></table></figure>

<p>生产者发送消息</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230302170001013.png" alt="image-20230302170001013"></p>
<p>消费者接受消息</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230302170027164.png" alt="image-20230302170027164"></p>
<h3 id="使用Kafka-Tools操作Kafka"><a href="#使用Kafka-Tools操作Kafka" class="headerlink" title="使用Kafka Tools操作Kafka"></a>使用Kafka Tools操作Kafka</h3><h4 id="连接Kafka集群"><a href="#连接Kafka集群" class="headerlink" title="连接Kafka集群"></a>连接Kafka集群</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230302170825532.png" alt="image-20230302170825532"></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230302170838039.png" alt="image-20230302170838039"></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230302170847729.png" alt="image-20230302170847729"></p>
<h4 id="创建topic-1"><a href="#创建topic-1" class="headerlink" title="创建topic"></a>创建topic</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230302170951476.png" alt="image-20230302170951476"></p>
<p>mac系统需要修改本地host,否则topic节点打不开</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/hosts</span><br><span class="line"></span><br><span class="line">10.211.55.9     ubuntu1 #prl_hostonly shared</span><br><span class="line">10.211.55.8     ubuntu2 #prl_hostonly shared</span><br><span class="line">10.211.55.7     ubuntu3 #prl_hostonly shared</span><br></pre></td></tr></table></figure>

<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230302193124105.png" alt="image-20230302193124105"></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230302193259933.png" alt="image-20230302193259933"></p>
<h2 id="Kafka基准测试"><a href="#Kafka基准测试" class="headerlink" title="Kafka基准测试"></a>Kafka基准测试</h2><h3 id="基准测试"><a href="#基准测试" class="headerlink" title="基准测试"></a>基准测试</h3><p>基准<a target="_blank" rel="noopener" href="http://www.blogjava.net/qileilove/archive/2012/07/05/382241.html">测试</a>（benchmark testing）是一种测量和评估软件性能指标的活动。我们可以通过基准测试，了解到软件、硬件的性能水平。主要测试负载的执行时间、传输速度、吞吐量、资源占用率等。</p>
<h4 id="基于1个分区1个副本的基准测试"><a href="#基于1个分区1个副本的基准测试" class="headerlink" title="基于1个分区1个副本的基准测试"></a>基于1个分区1个副本的基准测试</h4><ol>
<li>测试步骤：</li>
<li>启动Kafka集群</li>
<li>创建一个1个分区1个副本的topic: benchmark</li>
<li>同时运行生产者、消费者基准测试程序</li>
<li>观察结果</li>
</ol>
<h5 id="创建topic-2"><a href="#创建topic-2" class="headerlink" title="创建topic"></a>创建topic</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper ubuntu1:2181 --create --topic benchmark --partitions 1 --replication-factor 1</span><br></pre></td></tr></table></figure>

<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230302211748409.png" alt="image-20230302211748409"></p>
<h5 id="生产消息基准测试"><a href="#生产消息基准测试" class="headerlink" title="生产消息基准测试"></a>生产消息基准测试</h5><p>在生产环境中，推荐使用生产5000W消息，这样会性能数据会更准确些。为了方便测试，课程上演示测试500W的消息作为基准测试。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-producer-perf-test.sh --topic benchmark --num-records 5000000 --throughput -1 --record-size 1000 --producer-props bootstrap.servers=ubuntu1:9092,ubuntu2:9092,ubuntu3:9092 acks=1</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-producer-perf-test.sh </span><br><span class="line">--topic topic的名字</span><br><span class="line">--num-records	总共指定生产数据量（默认5000W）</span><br><span class="line">--throughput	指定吞吐量——限流（-1不指定）</span><br><span class="line">--record-size   record数据大小（字节）</span><br><span class="line">--producer-props bootstrap.servers=192.168.1.20:9092,192.168.1.21:9092,192.168.1.22:9092 acks=1 指定Kafka集群地址，ACK模式</span><br></pre></td></tr></table></figure>

<p>测试结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu1:/export/server/kafka_2.bin/kafka-producer-perf-test.sh --topic benchmark --num-records 5000000 --throughput -1 --record-size 1000 --producer-props bootstrap.servers=ubuntu1:9092,ubuntu2:9092,ubuntu3:9092 acks=1092 acks=1</span><br><span class="line">237649 records sent, 47529.8 records/sec (45.33 MB/sec), 597.6 ms avg latency, 819.0 ms max latency.</span><br><span class="line">221936 records sent, 44387.2 records/sec (42.33 MB/sec), 738.2 ms avg latency, 805.0 ms max latency.</span><br><span class="line">309552 records sent, 61910.4 records/sec (59.04 MB/sec), 540.4 ms avg latency, 785.0 ms max latency.</span><br><span class="line">291008 records sent, 58062.3 records/sec (55.37 MB/sec), 572.4 ms avg latency, 760.0 ms max latency.</span><br><span class="line">289072 records sent, 57814.4 records/sec (55.14 MB/sec), 562.1 ms avg latency, 782.0 ms max latency.</span><br><span class="line">450960 records sent, 90192.0 records/sec (86.01 MB/sec), 368.3 ms avg latency, 546.0 ms max latency.</span><br><span class="line">417168 records sent, 83433.6 records/sec (79.57 MB/sec), 392.7 ms avg latency, 646.0 ms max latency.</span><br><span class="line">443296 records sent, 88659.2 records/sec (84.55 MB/sec), 369.7 ms avg latency, 475.0 ms max latency.</span><br><span class="line">455920 records sent, 91184.0 records/sec (86.96 MB/sec), 354.5 ms avg latency, 640.0 ms max latency.</span><br><span class="line">476384 records sent, 95276.8 records/sec (90.86 MB/sec), 348.9 ms avg latency, 622.0 ms max latency.</span><br><span class="line">464896 records sent, 92979.2 records/sec (88.67 MB/sec), 351.9 ms avg latency, 574.0 ms max latency.</span><br><span class="line">454288 records sent, 90857.6 records/sec (86.65 MB/sec), 360.8 ms avg latency, 445.0 ms max latency.</span><br><span class="line">478912 records sent, 95782.4 records/sec (91.35 MB/sec), 341.4 ms avg latency, 399.0 ms max latency.</span><br><span class="line">5000000 records sent, 76782.505874 records/sec (73.23 MB/sec), 423.57 ms avg latency, 819.00 ms max latency, 354 ms 50th, 727 ms 95th, 779 ms 99th, 800 ms 99.9th.</span><br></pre></td></tr></table></figure>

<p>测试结果：</p>
<table>
<thead>
<tr>
<th>吞吐量</th>
<th><strong>76782.505874 records/sec   每秒9.3W条记录</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>吞吐速率</strong></td>
<td><strong>(73.23 MB/sec)  每秒约89MB数据</strong></td>
</tr>
<tr>
<td><strong>平均延迟时间</strong></td>
<td><strong>423.57 ms avg latency</strong></td>
</tr>
<tr>
<td><strong>最大延迟时间</strong></td>
<td><strong>819.00 ms max latency</strong></td>
</tr>
</tbody></table>
<h5 id="消费消息基准测试"><a href="#消费消息基准测试" class="headerlink" title="消费消息基准测试"></a>消费消息基准测试</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-perf-test.sh --broker-list ubuntu1:9092,ubuntu2:9092,ubuntu3:9092 --topic benchmark --fetch-size 1048576 --messages 5000000</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-perf-test.sh</span><br><span class="line">--broker-list 指定kafka集群地址</span><br><span class="line">--topic 指定topic的名称</span><br><span class="line">--fetch-size 每次拉取的数据大小</span><br><span class="line">--messages 总共要消费的消息个数</span><br></pre></td></tr></table></figure>

<p>测试结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu1:/export/server/kafka_2.bin/kafka-consumer-perf-test.sh --broker-list ubuntu1:9092,ubuntu2:9092,ubuntu3:9092 --topic benchmark --fetch-size 1048576 --messages 5000000es 500start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec, rebalance.time.ms, fetch.time.ms, fetch.MB.sec, fetch.nMsg.sec</span><br><span class="line">2023-03-02 13:32:03:115, 2023-03-02 13:32:16:727, 4768.3716, 350.3065, 5000000, 367322.9503, 1677763923405, -1677763909793, -0.0000, -0.0030</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>data.consumed.in.MB  共计消费的数据</th>
<th><strong>4768.3716MB</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>MB.sec  每秒消费的数量</strong></td>
<td><strong>350.3065  每秒350MB</strong></td>
</tr>
<tr>
<td><strong>data.consumed.in.nMsg  共计消费的数量</strong></td>
<td><strong>5000000</strong></td>
</tr>
<tr>
<td><strong>nMsg.sec  每秒的数量</strong></td>
<td><strong>367322.9503  每秒36.7W条</strong></td>
</tr>
</tbody></table>
<h4 id="基于3个分区1个副本的基准测试"><a href="#基于3个分区1个副本的基准测试" class="headerlink" title="基于3个分区1个副本的基准测试"></a>基于3个分区1个副本的基准测试</h4><h5 id="创建topic-3"><a href="#创建topic-3" class="headerlink" title="创建topic"></a>创建topic</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper ubuntu1:2181 --create --topic benchmark --partitions 3 --replication-factor 1</span><br></pre></td></tr></table></figure>

<h5 id="生产消息基准测试-1"><a href="#生产消息基准测试-1" class="headerlink" title="生产消息基准测试"></a>生产消息基准测试</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-producer-perf-test.sh --topic benchmark --num-records 5000000 --throughput -1 --record-size 1000 --producer-props bootstrap.servers=ubuntu1:9092,ubuntu2:9092,ubuntu3:9092 acks=1</span><br></pre></td></tr></table></figure>

<p>测试结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu1:/export/server/kafka_2.bin/kafka-producer-perf-test.sh --topic benchmark --num-records 5000000 --throughput -1 --record-size 1000 --producer-props bootstrap.servers=ubuntu1:9092,ubuntu2:9092,ubuntu3:9092 acks=1092 acks=1</span><br><span class="line">484241 records sent, 96848.2 records/sec (92.36 MB/sec), 288.7 ms avg latency, 703.0 ms max latency.</span><br><span class="line">450722 records sent, 90144.4 records/sec (85.97 MB/sec), 351.9 ms avg latency, 1714.0 ms max latency.</span><br><span class="line">812375 records sent, 162475.0 records/sec (154.95 MB/sec), 208.1 ms avg latency, 1160.0 ms max latency.</span><br><span class="line">934223 records sent, 186844.6 records/sec (178.19 MB/sec), 176.9 ms avg latency, 752.0 ms max latency.</span><br><span class="line">691132 records sent, 138226.4 records/sec (131.82 MB/sec), 237.0 ms avg latency, 839.0 ms max latency.</span><br><span class="line">706355 records sent, 141073.5 records/sec (134.54 MB/sec), 225.9 ms avg latency, 858.0 ms max latency.</span><br><span class="line">592279 records sent, 118455.8 records/sec (112.97 MB/sec), 277.3 ms avg latency, 1594.0 ms max latency.</span><br><span class="line">5000000 records sent, 133911.832450 records/sec (127.71 MB/sec), 240.60 ms avg latency, 1714.00 ms max latency, 18 ms 50th, 797 ms 95th, 1493 ms 99th, 1694 ms 99.9th.</span><br></pre></td></tr></table></figure>

<p>测试结果：</p>
<table>
<thead>
<tr>
<th><strong>指标</strong></th>
<th><strong>3</strong>分区1个副本</th>
<th><strong>单分区单副本</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>吞吐量</strong></td>
<td><strong>133911.832450 records/sec</strong></td>
<td><strong>76782.505874 records/sec   每秒9.3W条记录</strong></td>
</tr>
<tr>
<td><strong>吞吐速率</strong></td>
<td><strong>127.71 MB/sec</strong></td>
<td><strong>(73.23 MB/sec)  每秒约89MB数据</strong></td>
</tr>
<tr>
<td><strong>平均延迟时间</strong></td>
<td><strong>240.60 ms avg latency</strong></td>
<td><strong>423.57 ms avg latency</strong></td>
</tr>
<tr>
<td><strong>最大延迟时间</strong></td>
<td><strong>1714.00 ms max latency</strong></td>
<td><strong>819.00 ms max latency</strong></td>
</tr>
</tbody></table>
<p>在虚拟机上，因为都是共享笔记本上的CPU、内存、网络，所以分区越多，反而效率越低。但如果是真实的服务器，分区多效率是会有明显提升的。</p>
<h5 id="消费消息基准测试-1"><a href="#消费消息基准测试-1" class="headerlink" title="消费消息基准测试"></a>消费消息基准测试</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-perf-test.sh --broker-list ubuntu1:9092,ubuntu2:9092,ubuntu3:9092 --topic benchmark --fetch-size 1048576 --messages 5000000</span><br></pre></td></tr></table></figure>

<p>测试结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu1:/export/server/kafka_2.bin/kafka-consumer-perf-test.sh --broker-list ubuntu1:9092,ubuntu2:9092,ubuntu3:9092 --topic benchmark --fetch-size 1048576 --messages 5000000es 500start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec, rebalance.time.ms, fetch.time.ms, fetch.MB.sec, fetch.nMsg.sec</span><br><span class="line">2023-03-02 13:44:10:033, 2023-03-02 13:44:22:711, 4768.3716, 376.1139, 5000000, 394383.9722, 1677764650317, -1677764637639, -0.0000, -0.0030</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th><strong>指标</strong></th>
<th><strong>单分区3个副本</strong></th>
<th><strong>单分区单副本</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>data.consumed.in.MB  共计消费的数据</strong></td>
<td><strong>4768.3716</strong> MB</td>
<td><strong>4768.3716MB    4768.3716MB</strong></td>
</tr>
<tr>
<td><strong>MB.sec  每秒消费的数量</strong></td>
<td><strong>376.1139</strong></td>
<td><strong>350.3065  每秒350MB    445.6006  每秒445MB</strong></td>
</tr>
<tr>
<td><strong>data.consumed.in.nMsg  共计消费的数量</strong></td>
<td><strong>5000000</strong></td>
<td><strong>5000000</strong></td>
</tr>
<tr>
<td><strong>nMsg.sec  每秒的数量</strong></td>
<td><strong>394383.9722</strong></td>
<td><strong>367322.9503  每秒36.7W条</strong></td>
</tr>
</tbody></table>
<p>虽然是虚拟机 mac就是牛依然是提升的</p>
<h4 id="基于1个分区3个副本的基准测试"><a href="#基于1个分区3个副本的基准测试" class="headerlink" title="基于1个分区3个副本的基准测试"></a>基于1个分区3个副本的基准测试</h4><h5 id="创建topic-4"><a href="#创建topic-4" class="headerlink" title="创建topic"></a>创建topic</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper ubuntu1:2181 --create --topic benchmark --partitions 1 --replication-factor 3</span><br></pre></td></tr></table></figure>

<h5 id="生产消息基准测试-2"><a href="#生产消息基准测试-2" class="headerlink" title="生产消息基准测试"></a>生产消息基准测试</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-producer-perf-test.sh --topic benchmark --num-records 5000000 --throughput -1 --record-size 1000 --producer-props bootstrap.servers=ubuntu1:9092,ubuntu2:9092,ubuntu3:9092 acks=1</span><br></pre></td></tr></table></figure>

<p>测试结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu1:/export/server/kafka_2.bin/kafka-producer-perf-test.sh --topic benchmark --num-records 5000000 --throughput -1 --record-size 1000 --producer-props bootstrap.servers=ubuntu1:9092,ubuntu2:9092,ubuntu3:9092 acks=1092 acks=1</span><br><span class="line">145345 records sent, 29069.0 records/sec (27.72 MB/sec), 926.1 ms avg latency, 1293.0 ms max latency.</span><br><span class="line">242256 records sent, 48451.2 records/sec (46.21 MB/sec), 684.4 ms avg latency, 907.0 ms max latency.</span><br><span class="line">163568 records sent, 32713.6 records/sec (31.20 MB/sec), 979.1 ms avg latency, 1229.0 ms max latency.</span><br><span class="line">248480 records sent, 49696.0 records/sec (47.39 MB/sec), 675.7 ms avg latency, 971.0 ms max latency.</span><br><span class="line">229616 records sent, 45923.2 records/sec (43.80 MB/sec), 706.5 ms avg latency, 868.0 ms max latency.</span><br><span class="line">171936 records sent, 34387.2 records/sec (32.79 MB/sec), 840.2 ms avg latency, 1756.0 ms max latency.</span><br><span class="line">186592 records sent, 37318.4 records/sec (35.59 MB/sec), 982.8 ms avg latency, 1830.0 ms max latency.</span><br><span class="line">120368 records sent, 24064.0 records/sec (22.95 MB/sec), 1030.1 ms avg latency, 2565.0 ms max latency.</span><br><span class="line">187792 records sent, 37558.4 records/sec (35.82 MB/sec), 1073.2 ms avg latency, 2860.0 ms max latency.</span><br><span class="line">226480 records sent, 45296.0 records/sec (43.20 MB/sec), 748.3 ms avg latency, 1209.0 ms max latency.</span><br><span class="line">167728 records sent, 33545.6 records/sec (31.99 MB/sec), 948.7 ms avg latency, 1645.0 ms max latency.</span><br><span class="line">159312 records sent, 31862.4 records/sec (30.39 MB/sec), 1037.4 ms avg latency, 1410.0 ms max latency.</span><br><span class="line">134400 records sent, 26880.0 records/sec (25.63 MB/sec), 1225.2 ms avg latency, 2018.0 ms max latency.</span><br><span class="line">208432 records sent, 41678.1 records/sec (39.75 MB/sec), 779.9 ms avg latency, 886.0 ms max latency.</span><br><span class="line">185632 records sent, 37119.0 records/sec (35.40 MB/sec), 862.7 ms avg latency, 1511.0 ms max latency.</span><br><span class="line">202528 records sent, 40505.6 records/sec (38.63 MB/sec), 842.9 ms avg latency, 1159.0 ms max latency.</span><br><span class="line">237680 records sent, 47536.0 records/sec (45.33 MB/sec), 691.4 ms avg latency, 865.0 ms max latency.</span><br><span class="line">223792 records sent, 44678.0 records/sec (42.61 MB/sec), 715.1 ms avg latency, 863.0 ms max latency.</span><br><span class="line">229936 records sent, 45987.2 records/sec (43.86 MB/sec), 730.5 ms avg latency, 1010.0 ms max latency.</span><br><span class="line">260544 records sent, 52108.8 records/sec (49.69 MB/sec), 626.3 ms avg latency, 686.0 ms max latency.</span><br><span class="line">211824 records sent, 42364.8 records/sec (40.40 MB/sec), 779.5 ms avg latency, 1792.0 ms max latency.</span><br><span class="line">294144 records sent, 58828.8 records/sec (56.10 MB/sec), 561.3 ms avg latency, 661.0 ms max latency.</span><br><span class="line">322064 records sent, 64412.8 records/sec (61.43 MB/sec), 508.8 ms avg latency, 570.0 ms max latency.</span><br><span class="line">5000000 records sent, 41859.219074 records/sec (39.92 MB/sec), 777.09 ms avg latency, 2860.00 ms max latency, 705 ms 50th, 1245 ms 95th, 1978 ms 99th, 2840 ms 99.9th.</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th align="center"><strong>指标</strong></th>
<th><strong>单分区3个副本</strong></th>
<th><strong>单分区单副本</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>吞吐量</strong></td>
<td><strong>41859.219074 records/sec</strong></td>
<td><strong>76782.505874 records/sec   每秒9.3W条记录</strong></td>
</tr>
<tr>
<td align="center"><strong>吞吐速率</strong></td>
<td><strong>39.92 MB/sec</strong></td>
<td><strong>(73.23 MB/sec)  每秒约89MB数据</strong></td>
</tr>
<tr>
<td align="center"><strong>平均延迟时间</strong></td>
<td><strong>777.09 ms avg latency</strong></td>
<td><strong>423.57 ms avg latency</strong></td>
</tr>
<tr>
<td align="center"><strong>最大延迟时间</strong></td>
<td><strong>2860.00 ms max latency</strong></td>
<td><strong>819.00 ms max latency</strong></td>
</tr>
</tbody></table>
<p>同样的配置，副本越多速度越慢。</p>
<h5 id="消费消息基准测试-2"><a href="#消费消息基准测试-2" class="headerlink" title="消费消息基准测试"></a>消费消息基准测试</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-perf-test.sh --broker-list buntu1:9092,ubuntu2:9092,ubuntu3:9092 --topic benchmark --fetch-size 1048576 --messages 5000000</span><br></pre></td></tr></table></figure>

<p>测试结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu1:/export/server/kafka_2.bin/kafka-consumer-perf-test.sh --broker-list buntu1:9092,ubuntu2:9092,ubuntu3:9092 --topic benchmark --fetch-size 1048576 --messages 5000000es 5000start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec, rebalance.time.ms, fetch.time.ms, fetch.MB.sec, fetch.nMsg.sec</span><br><span class="line">[2023-03-02 13:55:39,901] WARN Couldn&#x27;t resolve server buntu1:9092 from bootstrap.servers as DNS resolution failed for buntu1 (org.apache.kafka.clients.ClientUtils)</span><br><span class="line">2023-03-02 13:55:39:945, 2023-03-02 13:55:55:180, 4768.3716, 312.9880, 5000000, 328191.6639, 1677765340146, -1677765324911, -0.0000, -0.0030</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th><strong>指标</strong></th>
<th><strong>单分区3个副本</strong></th>
<th><strong>单分区单副本</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>data.consumed.in.MB  共计消费的数据</strong></td>
<td><strong>4768.3716 MB</strong></td>
<td><strong>4768.3716MB    4768.3716MB</strong></td>
</tr>
<tr>
<td><strong>MB.sec  每秒消费的数量</strong></td>
<td><strong>312.9880</strong></td>
<td><strong>350.3065  每秒350MB    445.6006  每秒445MB</strong></td>
</tr>
<tr>
<td><strong>data.consumed.in.nMsg  共计消费的数量</strong></td>
<td><strong>5000000</strong></td>
<td><strong>5000000</strong></td>
</tr>
<tr>
<td><strong>nMsg.sec  每秒的数量</strong></td>
<td><strong>328191.6639</strong></td>
<td><strong>367322.9503  每秒36.7W条</strong></td>
</tr>
</tbody></table>
<h2 id="Java编程操作Kafka"><a href="#Java编程操作Kafka" class="headerlink" title="Java编程操作Kafka"></a>Java编程操作Kafka</h2><h3 id="同步生产消息到Kafka中"><a href="#同步生产消息到Kafka中" class="headerlink" title="同步生产消息到Kafka中"></a>同步生产消息到Kafka中</h3><h4 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h4><p>接下来，我们将编写Java程序，将1-100的数字消息写入到Kafka中。</p>
<h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><h5 id="导入Maven-Kafka-POM依赖"><a href="#导入Maven-Kafka-POM依赖" class="headerlink" title="导入Maven Kafka POM依赖"></a>导入Maven Kafka POM依赖</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span><span class="comment">&lt;!-- 代码库 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">id</span>&gt;</span>central<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public//<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">releases</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">releases</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">snapshots</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">updatePolicy</span>&gt;</span>always<span class="tag">&lt;/<span class="name">updatePolicy</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">checksumPolicy</span>&gt;</span>fail<span class="tag">&lt;/<span class="name">checksumPolicy</span>&gt;</span></span><br><span class="line"> 				<span class="tag">&lt;/<span class="name">snapshots</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- kafka客户端工具 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 工具类 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.commons<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-io<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- SLF桥接LOG4J日志 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- SLOG4J日志 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.16<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.7.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">        				<span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="导入log4j-properties"><a href="#导入log4j-properties" class="headerlink" title="导入log4j.properties"></a>导入log4j.properties</h5><p>将log4j.properties配置文件放入到resources文件夹中</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">log4j.rootLogger</span>=<span class="string">INFO,stdout</span></span><br><span class="line"><span class="attr">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender </span></span><br><span class="line"><span class="attr">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout </span></span><br><span class="line"><span class="attr">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%5p - %m%n</span></span><br></pre></td></tr></table></figure>

<h5 id="创建包和类"><a href="#创建包和类" class="headerlink" title="创建包和类"></a>创建包和类</h5><p>创建包<code>cn.itcast.kafka</code>，并创建<code>KafkaProducerTest</code>类。</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230303164943795.png" alt="image-20230303164943795"></p>
<h4 id="代码开发"><a href="#代码开发" class="headerlink" title="代码开发"></a>代码开发</h4><p>可以参考以下方式来编写第一个Kafka示例程序</p>
<p>参考以下文档：<a target="_blank" rel="noopener" href="http://kafka.apache.org/24/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html">http://kafka.apache.org/24/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html</a></p>
<ol>
<li>创建用于连接Kafka的Properties配置</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;192.168.88.100:9092&quot;</span>);</span><br><span class="line"><span class="comment">//这个配置是 Kafka 生产者和消费者必须要指定的一个配置项，它用于指定 Kafka 集群中的一个或多个 broker 地址，生产者和消费者将使用这些地址与 Kafka 集群建立连接。</span></span><br><span class="line">props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line"><span class="comment">//这行代码将 acks 配置设置为 all。acks 配置用于指定消息确认的级别。在此配置下，生产者将等待所有副本都成功写入后才会认为消息发送成功。这种配置级别可以确保数据不会丢失，但可能会影响性能。</span></span><br><span class="line">props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"><span class="comment">//这行代码将键（key）序列化器的类名设置为 org.apache.kafka.common.serialization.StringSerializer。键和值都需要被序列化以便于在网络上传输。这里使用的是一个字符串序列化器，它将字符串序列化为字节数组以便于发送到 Kafka 集群。</span></span><br><span class="line">props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"><span class="comment">//这行代码将值（value）序列化器的类名设置为 org.apache.kafka.common.serialization.StringSerializer。这里同样使用的是一个字符串序列化器，它将字符串序列化为字节数组以便于发送到 Kafka 集群。</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>创建一个生产者对象<code>KafkaProducer</code></li>
<li>调用send发送1-100消息到指定Topic test，并获取返回值Future，该对象封装了返回值</li>
<li>再调用一个<code>Future.get()</code>方法等待响应</li>
<li>关闭生产者</li>
</ol>
<p>参考代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaProducerTest</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1. 创建用于连接Kafka的Properties配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;192.168.88.100:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 创建一个生产者对象KafkaProducer</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 调用send发送1-100消息到指定Topic test</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; ++i) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// 获取返回值Future，该对象封装了返回值</span></span><br><span class="line">                Future&lt;RecordMetadata&gt; future = producer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;String, String&gt;(<span class="string">&quot;test&quot;</span>, <span class="literal">null</span>, i + <span class="string">&quot;&quot;</span>));</span><br><span class="line">              <span class="comment">//&quot;test&quot;：这个参数是指定 Kafka 主题（topic）的名称，表示这条记录将被发送到哪个主题中。</span></span><br><span class="line">              <span class="comment">//null：这个参数表示记录的键（key）。在 Kafka 中，每条消息都可以有一个键值对，键是一个可选参数，如果没有设置，则为 null。</span></span><br><span class="line">              <span class="comment">//i + &quot;&quot;：这个参数表示记录的值（value）。这里的 i 是一个整数，通过将它转换为字符串来设置记录的值。这个值将被序列化为字节数组并被发送到 Kafka 集群。</span></span><br><span class="line"></span><br><span class="line">综上所述，这行代码的含义是：创建一个 Kafka 生产者记录对象，将该记录的值设置为 i 的字符串形式，并指定该记录将被发送到名为 <span class="string">&quot;test&quot;</span> 的主题中，键为 <span class="literal">null</span>。</span><br><span class="line">                <span class="comment">// 调用一个Future.get()方法等待响应</span></span><br><span class="line">                future.get();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭生产者</span></span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>测试：</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230303165351697.png" alt="image-20230303165351697"></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230303165432474.png" alt="image-20230303165432474"></p>
<h3 id="从Kafka的topic中消费消息"><a href="#从Kafka的topic中消费消息" class="headerlink" title="从Kafka的topic中消费消息"></a>从Kafka的topic中消费消息</h3><h4 id="需求-1"><a href="#需求-1" class="headerlink" title="需求"></a>需求</h4><p>从 test topic中，将消息都消费，并将记录的offset、key、value打印出来</p>
<h4 id="准备工作-1"><a href="#准备工作-1" class="headerlink" title="准备工作"></a>准备工作</h4><p>在cn.itcast.kafka包下创建<code>KafkaConsumerTest</code>类</p>
<h4 id="开发步骤"><a href="#开发步骤" class="headerlink" title="开发步骤"></a>开发步骤</h4><ol>
<li>创建Kafka消费者配置</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1.itcast.cn:9092&quot;</span>);</span><br><span class="line"><span class="comment">//这一行将属性&quot;bootstrap.servers&quot;的值设置为&quot;node1.itcast.cn:9092&quot;。这是Kafka生产者和消费者所需的Kafka集群地址和端口号。</span></span><br><span class="line">props.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"><span class="comment">//这一行将属性&quot;group.id&quot;的值设置为&quot;test&quot;。这是消费者组的唯一标识符。所有属于同一组的消费者将共享一个消费者组ID。</span></span><br><span class="line">props.setProperty(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line"><span class="comment">//这一行将属性&quot;enable.auto.commit&quot;的值设置为&quot;true&quot;。这表示消费者是否应该自动提交偏移量。</span></span><br><span class="line">props.setProperty(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line"><span class="comment">//这一行将属性&quot;auto.commit.interval.ms&quot;的值设置为&quot;1000&quot;。这是消费者自动提交偏移量的时间间隔，以毫秒为单位。</span></span><br><span class="line">props.setProperty(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">props.setProperty(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"><span class="comment">//这两行将属性&quot;key.deserializer&quot;和&quot;value.deserializer&quot;的值都设置为&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;。这是用于反序列化Kafka消息的Java类的名称。在这种情况下，消息的键和值都是字符串类型，因此使用了StringDeserializer类来反序列化它们。</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>创建Kafka消费者</li>
<li>订阅要消费的主题</li>
<li>使用一个while循环，不断从Kafka的topic中拉取消息</li>
<li>将将记录（record）的offset、key、value都打印出来</li>
</ol>
<h4 id="参考代码"><a href="#参考代码" class="headerlink" title="参考代码"></a>参考代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.itcast.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaConsumerTest</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;10.211.55.8:9092&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建kafka消费者</span></span><br><span class="line">        <span class="type">KafkaConsumer</span> <span class="variable">kafkaConsumer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(props);</span><br><span class="line">        <span class="comment">//订阅要消费的主题</span></span><br><span class="line">        <span class="comment">//指定消费者从哪个topic中拉取数据</span></span><br><span class="line">        kafkaConsumer.subscribe(Arrays.asList(<span class="string">&quot;test&quot;</span>));</span><br><span class="line">        <span class="comment">//使用一个while循环，不断从kafka的topic中拉取消息</span></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">            <span class="comment">//kafka一次拉取一批数据</span></span><br><span class="line">            ConsumerRecords&lt;String,String&gt; poll = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">5</span>));</span><br><span class="line">            <span class="comment">//将记录record的offset、key、value都打印出来</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String,String&gt;consumerRecord:poll)&#123;</span><br><span class="line">                <span class="comment">//主题</span></span><br><span class="line">                <span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> consumerRecord.topic();</span><br><span class="line">                <span class="comment">//offset:这条消息处于kafka分区中的哪个位置</span></span><br><span class="line">                <span class="type">long</span> offset=consumerRecord.offset();</span><br><span class="line">                <span class="comment">//key\value</span></span><br><span class="line">                String key=consumerRecord.key();</span><br><span class="line">                String value=consumerRecord.value();</span><br><span class="line">                System.out.println(<span class="string">&quot;topic:&quot;</span>+topic+<span class="string">&quot;offset:&quot;</span>+offset+<span class="string">&quot;key:&quot;</span>+key+<span class="string">&quot;value:&quot;</span>+value);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>启动消费者，定位到最新的offset</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230303174132218.png" alt="image-20230303174132218"></p>
<p>生产者再次发送消息，观察消费者变化</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230303174248747.png" alt="image-20230303174248747"></p>
<h3 id="异步使用带有回调函数方法生产消息"><a href="#异步使用带有回调函数方法生产消息" class="headerlink" title="异步使用带有回调函数方法生产消息"></a>异步使用带有回调函数方法生产消息</h3><p>如果我们想获取生产者消息是否成功，或者成功生产消息到<code>Kafka</code>中后，执行一些其他动作。此时，可以很方便地使用带有回调函数来发送消息。</p>
<p>需求：</p>
<ol>
<li>在发送消息出现异常时，能够及时打印出异常信息</li>
<li>在发送消息成功时，打印Kafka的topic名字、分区id、offset</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.itcast.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Callback;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.Future;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaProducerTest</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1. 创建用于连接Kafka的Properties配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1.itcast.cn:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 创建一个生产者对象KafkaProducer</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 调用send发送1-100消息到指定Topic test</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; ++i) &#123;</span><br><span class="line">            <span class="comment">// 一、同步方式</span></span><br><span class="line">            <span class="comment">// 获取返回值Future，该对象封装了返回值</span></span><br><span class="line">            <span class="comment">// Future&lt;RecordMetadata&gt; future = producer.send(new ProducerRecord&lt;String, String&gt;(&quot;test&quot;, null, i + &quot;&quot;));</span></span><br><span class="line">            <span class="comment">// 调用一个Future.get()方法等待响应</span></span><br><span class="line">            <span class="comment">// future.get();</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">// 二、带回调函数异步方式</span></span><br><span class="line">            producer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;String, String&gt;(<span class="string">&quot;test&quot;</span>, <span class="literal">null</span>, i + <span class="string">&quot;&quot;</span>), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span>(exception != <span class="literal">null</span>) &#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;发送消息出现异常&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> metadata.topic();</span><br><span class="line">                        <span class="type">int</span> <span class="variable">partition</span> <span class="operator">=</span> metadata.partition();</span><br><span class="line">                        <span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> metadata.offset();</span><br><span class="line"></span><br><span class="line">                        System.out.println(<span class="string">&quot;发送消息到Kafka中的名字为&quot;</span> + topic + <span class="string">&quot;的主题，第&quot;</span> + partition + <span class="string">&quot;分区，第&quot;</span> + offset + <span class="string">&quot;条数据成功!&quot;</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5. 关闭生产者</span></span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304125250709.png" alt="image-20230304125250709"></p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><h3 id="Kafka重要概念"><a href="#Kafka重要概念" class="headerlink" title="Kafka重要概念"></a>Kafka重要概念</h3><h4 id="broker"><a href="#broker" class="headerlink" title="broker"></a>broker</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304125417711.png" alt="image-20230304125417711"></p>
<ol>
<li> 一个Kafka的集群通常由多个<code>broker</code>组成，这样才能实现负载均衡、以及容错</li>
<li><code>broker</code>是<code>无状态（Sateless</code>的，它们是通过<code>ZooKeeper</code>来维护集群状态</li>
<li>一个Kafka的<code>broker</code>每秒可以处理数十万次读写，每个<code>broker</code>都可以处理TB消息而不影响性能</li>
</ol>
<h4 id="zookeeper"><a href="#zookeeper" class="headerlink" title="zookeeper"></a>zookeeper</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304125745530.png" alt="image-20230304125745530"></p>
<ol>
<li>ZK用来管理和协调<code>broker</code>，并且存储了Kafka的元数据（例如：有多少topic、partition、consumer）</li>
<li>ZK服务主要用于通知生产者和消费者Kafka集群中有新的<code>broker</code>加入、或者Kafka集群中出现故障的<code>broker</code>。</li>
</ol>
<p>PS：Kafka正在逐步想办法将ZooKeeper剥离，维护两套集群成本较高，社区提出KIP-500就是要替换掉ZooKeeper的依赖。“Kafka on Kafka”——Kafka自己来管理自己的元数据</p>
<h4 id="producer（生产者）"><a href="#producer（生产者）" class="headerlink" title="producer（生产者）"></a>producer（生产者）</h4><p>生产者负责将数据推送给<code>broker</code>的<code>topic</code></p>
<h4 id="consumer（消费者）"><a href="#consumer（消费者）" class="headerlink" title="consumer（消费者）"></a>consumer（消费者）</h4><p>消费者负责从<code>broker</code>的<code>topic</code>中拉取数据，并自己进行处理</p>
<h4 id="consumer-group（消费者组）"><a href="#consumer-group（消费者组）" class="headerlink" title="consumer group（消费者组）"></a>consumer group（消费者组）</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304125938966.png" alt="image-20230304125938966"></p>
<ul>
<li><code>consumer group</code>是kafka提供的可扩展且具有容错性的消费者机制</li>
<li>一个消费者组可以包含多个消费者</li>
<li>一个消费者组有一个唯一的ID（group Id）</li>
<li>组内的消费者一起消费主题的所有分区数据</li>
</ul>
<h4 id="分区（Partitions）"><a href="#分区（Partitions）" class="headerlink" title="分区（Partitions）"></a>分区（Partitions）</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304130211916.png" alt="image-20230304130211916"></p>
<p>在Kafka集群中，主题被分为多个分区</p>
<p>在 Kafka 中，同一个 topic 的消息可以被分配到不同的分区中，具体分配规则取决于 partitioner。</p>
<p>Kafka 提供了默认的 partitioner 实现，称为 DefaultPartitioner，其将消息的 key（如果存在）进行哈希，然后根据哈希值确定该消息应该被分配到哪个分区。如果消息没有 key，则采用轮询的方式将消息分配到不同的分区中。</p>
<p>除了默认的 partitioner，用户还可以自定义 partitioner 实现，以满足不同的需求。自定义 partitioner 实现需要实现 Kafka 提供的 Partitioner 接口，并在生产者配置中指定使用该 partitioner。</p>
<p>无论是使用默认的 partitioner 还是自定义 partitioner，都需要遵循以下规则：</p>
<ul>
<li>对于同一个 key，始终分配到同一个分区中。</li>
<li>对于没有 key 的消息，应该采用随机或轮询的方式将消息分配到不同的分区中。</li>
</ul>
<p>需要注意的是，分区数的变化也可能导致消息分配到不同的分区中。例如，当某个 topic 的分区数发生变化时，之前已经写入的消息可能会被重新分配到不同的分区中。因此，在生产者代码中应该谨慎处理分区数的变化，以避免数据丢失或重复。</p>
<h4 id="副本（Replicas）"><a href="#副本（Replicas）" class="headerlink" title="副本（Replicas）"></a>副本（Replicas）</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304130602885.png" alt="image-20230304130602885"></p>
<ul>
<li>副本可以确保某个服务器出现故障时，确保数据依然可用</li>
<li>在Kafka中，一般都会设计副本的个数＞1</li>
</ul>
<h4 id="主题（Topic）"><a href="#主题（Topic）" class="headerlink" title="主题（Topic）"></a>主题（Topic）</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304130757615.png" alt="image-20230304130757615"></p>
<ul>
<li>主题是一个逻辑概念，用于生产者发布数据，消费者拉取数据</li>
<li>Kafka中的主题必须要有标识符，而且是唯一的，Kafka中可以有任意数量的主题，没有数量上的限制</li>
<li>在主题中的消息是有结构的，一般一个主题包含某一类消息</li>
<li>一旦生产者发送消息到主题中，这些消息就不能被更新（更改）</li>
</ul>
<h4 id="偏移量（offset）"><a href="#偏移量（offset）" class="headerlink" title="偏移量（offset）"></a>偏移量（offset）</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304133000845.png" alt="image-20230304133000845"></p>
<p><strong>offset</strong>记录着下一条将要发送给Consumer的消息的序号</p>
<p><strong>默认Kafka将offset存储在ZooKeeper中</strong></p>
<p>在一个分区中，消息是有顺序的方式存储着，每个在分区的消费都是有一个递增的id。这个就是偏移量offset</p>
<p>偏移量在分区中才是有意义的。在分区之间，offset是没有任何意义的</p>
<h3 id="消费者组"><a href="#消费者组" class="headerlink" title="消费者组"></a>消费者组</h3><p>Kafka支持有多个消费者同时消费一个主题中的数据。我们接下来，给大家演示，启动两个消费者共同来消费 test 主题的数据。</p>
<ol>
<li>首先，修改生产者程序，让生产者不停地每3秒生产1-100个数字。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 3. 发送1-100数字到Kafka的test主题中</span></span><br><span class="line"><span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt;= <span class="number">100</span>; ++i) &#123;</span><br><span class="line">        <span class="comment">// 注意：send方法是一个异步方法，它会将要发送的数据放入到一个buffer中，然后立即返回</span></span><br><span class="line">        <span class="comment">// 这样可以让消息发送变得更高效</span></span><br><span class="line">        producer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;test&quot;</span>, i + <span class="string">&quot;&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    Thread.sleep(<span class="number">3000</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>接下来，同时运行两个消费者。</li>
</ol>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304214510037.png" alt="image-20230304214510037"></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304214737410.png" alt="image-20230304214737410"></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304214813494.png" alt="image-20230304214813494"></p>
<ol start="3">
<li>同时运行两个消费者，我们发现，只有一个消费者程序能够拉取到消息。想要让两个消费者同时消费消息，必须要给test主题，添加一个分区。</li>
</ol>
<p># 设置 test topic为2个分区</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper <span class="number">10.211</span><span class="number">.55</span><span class="number">.8</span>:<span class="number">2181</span> -alter --partitions <span class="number">2</span> --topic test</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">xuwei@ubuntu1:/export/server/kafka_2.12-2.4.1$ bin/kafka-topics.sh --zookeeper 10.211.55.8:2181 -alter --partitions 2 --topic test</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line">Adding partitions succeeded!</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>重新运行生产者、两个消费者程序，我们就可以看到两个消费者都可以消费Kafka Topic的数据了</li>
</ol>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304215343589.png" alt="image-20230304215343589"></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304215357086.png" alt="image-20230304215357086"></p>
<h2 id="Kafka生产者幂等性与事务"><a href="#Kafka生产者幂等性与事务" class="headerlink" title="Kafka生产者幂等性与事务"></a>Kafka生产者幂等性与事务</h2><h3 id="幂等性"><a href="#幂等性" class="headerlink" title="幂等性"></a>幂等性</h3><h4 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h4><p>拿http举例来说，一次或多次请求，得到地响应是一致的（网络超时等问题除外），换句话说，就是执行多次操作与执行一次操作的影响是一样的。</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304221941602.png" alt="image-20230304221941602"></p>
<p>如果，某个系统是不具备幂等性的，如果用户重复提交了某个表格，就可能会造成不良影响。例如：用户在浏览器上点击了多次提交订单按钮，会在后台生成多个一模一样的订单。</p>
<h4 id="Kafka生产者幂等性"><a href="#Kafka生产者幂等性" class="headerlink" title="Kafka生产者幂等性"></a>Kafka生产者幂等性</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304222141263.png" alt="image-20230304222141263"></p>
<p>在生产者生产消息时，如果出现<code>retry</code>时，有可能会一条消息被发送了多次，如果Kafka不具备幂等性的，就有可能会在<code>partition</code>中保存多条一模一样的消息。</p>
<h4 id="配置幂等性"><a href="#配置幂等性" class="headerlink" title="配置幂等性"></a>配置幂等性</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(<span class="string">&quot;enable.idempotence&quot;</span>,<span class="literal">true</span>);</span><br></pre></td></tr></table></figure>

<h4 id="幂等性原理"><a href="#幂等性原理" class="headerlink" title="幂等性原理"></a>幂等性原理</h4><p>为了实现生产者的幂等性，Kafka引入了 <code>Producer ID（PID）</code>和 <code>Sequence Number</code>的概念。</p>
<p> <code>PID</code>：每个Producer在初始化时，都会分配一个唯一的PID，这个PID对用户来说，是透明的。</p>
<p> <code>Sequence Number</code>：针对每个生产者（对应PID）发送到指定主题分区的消息都对应一个从0开始递增的<code>Sequence Number</code>。</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304222724389.png" alt="image-20230304222724389"></p>
<h3 id="Kafka事务"><a href="#Kafka事务" class="headerlink" title="Kafka事务"></a>Kafka事务</h3><h4 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h4><p>Kafka事务是2017年Kafka 0.11.0.0引入的新特性。类似于数据库的事务。Kafka事务指的是生产者生产消息以及消费者提交offset的操作可以在一个原子操作中，要么都成功，要么都失败。尤其是在生产者、消费者并存时，事务的保障尤其重要。（consumer-transform-producer模式）</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304222942728.png" alt="image-20230304222942728"></p>
<h4 id="事务操作API"><a href="#事务操作API" class="headerlink" title="事务操作API"></a>事务操作API</h4><p><code>Producer</code>接口中定义了以下5个事务相关方法：</p>
<ol>
<li><p><code>initTransactions</code>（初始化事务）：要使用Kafka事务，必须先进行初始化操作</p>
</li>
<li><p><code>beginTransaction</code>（开始事务）：启动一个Kafka事务</p>
</li>
<li><p><code>sendOffsetsToTransaction</code>（提交偏移量）：批量地将分区对应的offset发送到事务中，方便后续一块提交</p>
</li>
<li><p><code>commitTransaction</code>（提交事务）：提交事务</p>
</li>
<li><p><code>abortTransaction</code>（放弃事务）：取消事务</p>
</li>
</ol>
<h3 id="Kafka事务编程"><a href="#Kafka事务编程" class="headerlink" title="Kafka事务编程"></a>Kafka事务编程</h3><h4 id="事务相关属性配置"><a href="#事务相关属性配置" class="headerlink" title="事务相关属性配置"></a>事务相关属性配置</h4><h5 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 配置事务的id，开启了事务会默认开启幂等性</span></span><br><span class="line">props.put(<span class="string">&quot;transactional.id&quot;</span>, <span class="string">&quot;first-transactional&quot;</span>);</span><br></pre></td></tr></table></figure>

<h5 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 消费者需要设置隔离级别</span></span><br><span class="line">props.put(<span class="string">&quot;isolation.level&quot;</span>,<span class="string">&quot;read_committed&quot;</span>);</span><br><span class="line"><span class="comment">//  2. 关闭自动提交</span></span><br><span class="line">props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);</span><br></pre></td></tr></table></figure>

<h4 id="Kafka事务编程-1"><a href="#Kafka事务编程-1" class="headerlink" title="Kafka事务编程"></a>Kafka事务编程</h4><h5 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.itcast.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">AffairTest</span> &#123;</span><br><span class="line">    <span class="comment">// 1. 创建消费者</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Consumer&lt;String, String&gt; <span class="title function_">createConsumer</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 1. 创建Kafka消费者配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;10.211.55.8:9092&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;ods_user&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;isolation.level&quot;</span>,<span class="string">&quot;read_committed&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 创建Kafka消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 订阅要消费的主题</span></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;ods_user&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> consumer;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Producer&lt;String, String&gt; <span class="title function_">createProducer</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 1. 创建生产者配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;10.211.55.8:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;transactional.id&quot;</span>, <span class="string">&quot;dwd_user&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 创建生产者</span></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(props);</span><br><span class="line">        <span class="keyword">return</span> producer;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        Consumer&lt;String, String&gt; consumer = createConsumer();</span><br><span class="line">        Producer&lt;String, String&gt; producer = createProducer();</span><br><span class="line">        <span class="comment">// 初始化事务</span></span><br><span class="line">        producer.initTransactions();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// 1. 开启事务</span></span><br><span class="line">                producer.beginTransaction();</span><br><span class="line">                <span class="comment">// 2. 定义Map结构，用于保存分区对应的offset</span></span><br><span class="line">                Map&lt;TopicPartition, OffsetAndMetadata&gt; offsetCommits = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">                <span class="comment">// 2. 拉取消息</span></span><br><span class="line">                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(<span class="number">2</span>));</span><br><span class="line">                <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                    <span class="comment">// 3. 保存偏移量</span></span><br><span class="line">                    offsetCommits.put(<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(record.topic(), record.partition()),</span><br><span class="line">                            <span class="keyword">new</span> <span class="title class_">OffsetAndMetadata</span>(record.offset() + <span class="number">1</span>));</span><br><span class="line">                    <span class="comment">// 4. 进行转换处理</span></span><br><span class="line">                    String[] fields = record.value().split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                    fields[<span class="number">1</span>] = fields[<span class="number">1</span>].equalsIgnoreCase(<span class="string">&quot;1&quot;</span>) ? <span class="string">&quot;男&quot;</span>:<span class="string">&quot;女&quot;</span>;</span><br><span class="line">                    <span class="type">String</span> <span class="variable">message</span> <span class="operator">=</span> fields[<span class="number">0</span>] + <span class="string">&quot;,&quot;</span> + fields[<span class="number">1</span>] + <span class="string">&quot;,&quot;</span> + fields[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 5. 生产消息到dwd_user</span></span><br><span class="line">                    producer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;dwd_user&quot;</span>, message));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 6. 提交偏移量到事务</span></span><br><span class="line">                producer.sendOffsetsToTransaction(offsetCommits, <span class="string">&quot;ods_user&quot;</span>);</span><br><span class="line">                <span class="comment">// 7. 提交事务</span></span><br><span class="line">                producer.commitTransaction();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                <span class="comment">// 8. 放弃事务</span></span><br><span class="line">                producer.abortTransaction();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="需求-2"><a href="#需求-2" class="headerlink" title="需求"></a>需求</h5><p>在Kafka的topic 「ods_user」中有一些用户数据，数据格式如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">姓名,性别,出生日期</span><br><span class="line">张三,<span class="number">1</span>,<span class="number">1980</span>-<span class="number">10</span>-09</span><br><span class="line">李四,<span class="number">0</span>,<span class="number">1985</span>-<span class="number">11</span>-<span class="number">01</span></span><br></pre></td></tr></table></figure>

<p>我们需要编写程序，将用户的性别转换为男、女（1-男，0-女），转换后将数据写入到topic 「dwd_user」中。要求使用事务保障，要么消费了数据同时写入数据到 topic，提交offset。要么全部失败。</p>
<h5 id="启动生产者控制台程序模拟数据"><a href="#启动生产者控制台程序模拟数据" class="headerlink" title="启动生产者控制台程序模拟数据"></a>启动生产者控制台程序模拟数据</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建名为ods_user和dwd_user的主题</span></span><br><span class="line">bin/kafka-topics.sh --create --bootstrap-server 10.211.55.8:9092 --topic ods_user</span><br><span class="line">bin/kafka-topics.sh --create --bootstrap-server 10.211.55.8:9092 --topic dwd_user</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">生产数据到 ods_user</span></span><br><span class="line">bin/kafka-console-producer.sh --broker-list 10.211.55.8:9092 --topic ods_user</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">从dwd_user消费数据</span></span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server 10.211.55.8:9092 --topic dwd_user --from-beginning  --isolation-level read_committed</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304233913158.png" alt="image-20230304233913158"></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230304234018515.png" alt="image-20230304234018515"></p>
<h5 id="编写创建消费者代码"><a href="#编写创建消费者代码" class="headerlink" title="编写创建消费者代码"></a>编写创建消费者代码</h5><p>编写一个方法 createConsumer，该方法中返回一个消费者，订阅「ods_user」主题。注意：需要配置事务隔离级别、关闭自动提交。</p>
<p>创建消费者，并订阅 ods_user 主题</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 创建消费者</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Consumer&lt;String, String&gt; <span class="title function_">createConsumer</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 1. 创建Kafka消费者配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;10.211.55.8:9092&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;ods_user&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;isolation.level&quot;</span>,<span class="string">&quot;read_committed&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 创建Kafka消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 订阅要消费的主题</span></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;ods_user&quot;</span>));</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> consumer;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="编写创建生产者代码"><a href="#编写创建生产者代码" class="headerlink" title="编写创建生产者代码"></a>编写创建生产者代码</h5><p>编写一个方法 createProducer，返回一个生产者对象。注意：需要配置事务的id，开启了事务会默认开启幂等性。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Producer&lt;String, String&gt; <span class="title function_">createProduceer</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 1. 创建生产者配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;10.211.55.8:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;transactional.id&quot;</span>, <span class="string">&quot;dwd_user&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 创建生产者</span></span><br><span class="line">        Producer&lt;String, String&gt; producer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(props);</span><br><span class="line">        <span class="keyword">return</span> producer;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="编写代码消费并生产数据"><a href="#编写代码消费并生产数据" class="headerlink" title="编写代码消费并生产数据"></a>编写代码消费并生产数据</h5><p>实现步骤：</p>
<ol>
<li><p>调用之前实现的方法，创建消费者、生产者对象</p>
</li>
<li><p>生产者调用initTransactions初始化事务</p>
</li>
<li><p>编写一个while死循环，在while循环中不断拉取数据，进行处理后，再写入到指定的topic</p>
</li>
</ol>
<p>(1)   生产者开启事务</p>
<p>(2)   消费者拉取消息</p>
<p>(3)   遍历拉取到的消息，并进行预处理（将1转换为男，0转换为女）</p>
<p>(4)   生产消息到dwd_user topic中</p>
<p>(5)   提交偏移量到事务中</p>
<p>(6)   提交事务</p>
<p>(7)   捕获异常，如果出现异常，则取消事务</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        Consumer&lt;String, String&gt; consumer = createConsumer();</span><br><span class="line">        Producer&lt;String, String&gt; producer = createProducer();</span><br><span class="line">        <span class="comment">// 初始化事务</span></span><br><span class="line">        producer.initTransactions();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// 1. 开启事务</span></span><br><span class="line">                producer.beginTransaction();</span><br><span class="line">                <span class="comment">// 2. 定义Map结构，用于保存分区对应的offset</span></span><br><span class="line">                Map&lt;TopicPartition, OffsetAndMetadata&gt; offsetCommits = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">                <span class="comment">// 2. 拉取消息</span></span><br><span class="line">                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(<span class="number">2</span>));</span><br><span class="line">                <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                    <span class="comment">// 3. 保存偏移量</span></span><br><span class="line">                    offsetCommits.put(<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(record.topic(), record.partition()),</span><br><span class="line">                            <span class="keyword">new</span> <span class="title class_">OffsetAndMetadata</span>(record.offset() + <span class="number">1</span>));</span><br><span class="line">                  <span class="comment">//将当前消息所属分区的偏移量保存到HashMap中，并且将偏移量加1，以便下次从此偏移量开始消费消息。</span></span><br><span class="line">                    <span class="comment">// 4. 进行转换处理</span></span><br><span class="line">                    String[] fields = record.value().split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">                    fields[<span class="number">1</span>] = fields[<span class="number">1</span>].equalsIgnoreCase(<span class="string">&quot;1&quot;</span>) ? <span class="string">&quot;男&quot;</span>:<span class="string">&quot;女&quot;</span>;</span><br><span class="line">                    <span class="type">String</span> <span class="variable">message</span> <span class="operator">=</span> fields[<span class="number">0</span>] + <span class="string">&quot;,&quot;</span> + fields[<span class="number">1</span>] + <span class="string">&quot;,&quot;</span> + fields[<span class="number">2</span>];</span><br><span class="line">                    <span class="comment">// 5. 生产消息到dwd_user</span></span><br><span class="line">                    producer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;dwd_user&quot;</span>, message));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 6. 提交偏移量到事务，</span></span><br><span class="line">                producer.sendOffsetsToTransaction(offsetCommits, <span class="string">&quot;ods_user&quot;</span>);</span><br><span class="line">                <span class="comment">// 7. 提交事务</span></span><br><span class="line">                producer.commitTransaction();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                <span class="comment">// 8. 放弃事务</span></span><br><span class="line">                producer.abortTransaction();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>将已消费的消息的偏移量提交到生产者的事务中，是为了确保在生产者发送消息到新的主题之前，已经消费的消息的偏移量已经被记录下来并保存在事务中。如果不提交偏移量，则可能会导致已经消费的消息在下一次启动消费者时重复消费。因此，将偏移量提交到生产者的事务中是非常重要的，可以确保消费者在下一次启动时可以正确地从上次停止的位置继续消费。</p>
<h5 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h5><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305000317993.png" alt="image-20230305000317993"></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305000350416.png" alt="image-20230305000350416"></p>
<p>成功转化并消费</p>
<h5 id="模拟异常测试事务"><a href="#模拟异常测试事务" class="headerlink" title="模拟异常测试事务"></a>模拟异常测试事务</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 3. 保存偏移量</span></span><br><span class="line">offsetCommits.put(<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(record.topic(), record.partition()),</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">OffsetAndMetadata</span>(record.offset() + <span class="number">1</span>));</span><br><span class="line"><span class="comment">// 4. 进行转换处理</span></span><br><span class="line">String[] fields = record.value().split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">fields[<span class="number">1</span>] = fields[<span class="number">1</span>].equalsIgnoreCase(<span class="string">&quot;1&quot;</span>) ? <span class="string">&quot;男&quot;</span>:<span class="string">&quot;女&quot;</span>;</span><br><span class="line"><span class="type">String</span> <span class="variable">message</span> <span class="operator">=</span> fields[<span class="number">0</span>] + <span class="string">&quot;,&quot;</span> + fields[<span class="number">1</span>] + <span class="string">&quot;,&quot;</span> + fields[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 模拟异常</span></span><br><span class="line"><span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>/<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5. 生产消息到dwd_user</span></span><br><span class="line">producer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;dwd_user&quot;</span>, message));</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>启动程序一次，抛出异常。</p>
<p>再启动程序一次，还是抛出异常。</p>
<p>直到我们处理该异常为止。</p>
<p> <img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305000812268.png" alt="image-20230305000812268"></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305000838852.png" alt="image-20230305000838852"></p>
<p>我们发现，可以消费到消息，但如果中间出现异常的话，offset是不会被提交的，除非消费、生产消息都成功，才会提交事务。</p>
<p><strong>Kafka高级</strong></p>
<p><strong>学习目标</strong></p>
<ul>
<li>理解Kafka的分区副本机制 </li>
<li>能够搭建Kafka-eagle并查看Kafka集群状态</li>
<li>理解分区leader和follower的职责</li>
<li>理解分区的ISR</li>
<li>理解Kafka消息不丢失机制</li>
<li>理解Kafka中数据清理</li>
</ul>
<h2 id="分区和副本机制"><a href="#分区和副本机制" class="headerlink" title="分区和副本机制"></a>分区和副本机制</h2><h3 id="生产者分区写入策略"><a href="#生产者分区写入策略" class="headerlink" title="生产者分区写入策略"></a>生产者分区写入策略</h3><p>生产者写入消息到topic，Kafka将依据不同的策略将数据分配到不同的分区中</p>
<ol>
<li><p>轮询分区策略</p>
</li>
<li><p>随机分区策略</p>
</li>
<li><p>按key分区分配策略</p>
</li>
<li><p>自定义分区策略</p>
</li>
</ol>
<h4 id="轮询策略"><a href="#轮询策略" class="headerlink" title="轮询策略"></a>轮询策略</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305211026078.png" alt="image-20230305211026078"></p>
<ul>
<li>默认的策略，也是使用最多的策略，可以最大限度保证所有消息平均分配到一个分区</li>
<li>如果在生产消息时，key为null，则使用轮询算法均衡地分配分区</li>
</ul>
<h4 id="随机策略（不用）"><a href="#随机策略（不用）" class="headerlink" title="随机策略（不用）"></a>随机策略（不用）</h4><p>随机策略，每次都随机地将消息分配到每个分区。在较早的版本，默认的分区策略就是随机策略，也是为了将消息均衡地写入到每个分区。但后续轮询策略表现更佳，所以基本上很少会使用随机策略。</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305211117624.png" alt="image-20230305211117624"></p>
<h4 id="按key分配策略"><a href="#按key分配策略" class="headerlink" title="按key分配策略"></a>按key分配策略</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305211144842.png" alt="image-20230305211144842"></p>
<p>按key分配策略，有可能会出现「数据倾斜」，例如：某个key包含了大量的数据，因为key值一样，所有所有的数据将都分配到一个分区中，造成该分区的消息数量远大于其他的分区。</p>
<h4 id="乱序问题"><a href="#乱序问题" class="headerlink" title="乱序问题"></a>乱序问题</h4><p>轮询策略、随机策略都会导致一个问题，生产到Kafka中的数据是乱序存储的。而按key分区可以一定程度上实现数据有序存储——也就是局部有序，但这又可能会导致数据倾斜，所以在实际生产环境中要结合实际情况来做取舍。</p>
<h4 id="自定义分区策略"><a href="#自定义分区策略" class="headerlink" title="自定义分区策略"></a>自定义分区策略</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305211507494.png" alt="image-20230305211507494"></p>
<p>实现步骤：</p>
<ol>
<li>创建自定义分区器</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KeyWithRandomPartitioner</span> <span class="keyword">implements</span> <span class="title class_">Partitioner</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Random r;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> &#123;</span><br><span class="line">        r = <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">partition</span><span class="params">(String topic, Object key, <span class="type">byte</span>[] keyBytes, Object value, <span class="type">byte</span>[] valueBytes, Cluster cluster)</span> &#123;</span><br><span class="line">        <span class="comment">// cluster.partitionCountForTopic 表示获取指定topic的分区数量r.nextInt(1000) 表示从随机数生成器 r 中随机生成一个小于1000的整数，其中参数1000指定了生成的随机数的范围，即生成的随机数是0到999之间的整数。在这段代码中，生成的随机数将被用于计算消息所在的分区编号。由于模运算 % cluster.partitionCountForTopic(topic) 的结果必须小于分区数量，因此这里对1000取模的目的是将随机数的范围缩小到分区数量内，以确保不会选择到超出范围的分区编号。</span></span><br><span class="line">        <span class="keyword">return</span> r.nextInt(<span class="number">1000</span>) % cluster.partitionCountForTopic(topic);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ol>
<li>在Kafka生产者配置中，自定使用自定义分区器的类名</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, KeyWithRandomPartitioner.class.getName());</span><br></pre></td></tr></table></figure>

<h3 id="消费者组Rebalance机制"><a href="#消费者组Rebalance机制" class="headerlink" title="消费者组Rebalance机制"></a>消费者组Rebalance机制</h3><h4 id="Rebalance再均衡"><a href="#Rebalance再均衡" class="headerlink" title="Rebalance再均衡"></a>Rebalance再均衡</h4><p>Kafka中的Rebalance称之为再均衡，是Kafka中确保Consumer group下所有的consumer如何达成一致，分配订阅的topic的每个分区的机制。</p>
<p>Rebalance触发的时机有：</p>
<ol>
<li>消费者组中consumer的个数发生变化。例如：有新的consumer加入到消费者组，或者是某个consumer停止了。</li>
</ol>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305213339845.png" alt="image-20230305213339845"></p>
<ol start="2">
<li>订阅的topic个数发生变化</li>
</ol>
<p>消费者可以订阅多个主题，假设当前的消费者组订阅了三个主题，但有一个主题突然被删除了，此时也需要发生再均衡。</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305213419840.png" alt="image-20230305213419840"></p>
<ol start="3">
<li>订阅的topic分区数发生变化</li>
</ol>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305213443651.png" alt="image-20230305213443651"></p>
<h4 id="Rebalance的不良影响"><a href="#Rebalance的不良影响" class="headerlink" title="Rebalance的不良影响"></a>Rebalance的不良影响</h4><ul>
<li>发生Rebalance时，consumer group下的所有consumer都会协调在一起共同参与，Kafka使用分配策略尽可能达到最公平的分配</li>
<li>Rebalance过程会对consumer group产生非常严重的影响，Rebalance的过程中所有的消费者都将停止工作，直到Rebalance完成</li>
</ul>
<h3 id="消费者分区分配策略"><a href="#消费者分区分配策略" class="headerlink" title="消费者分区分配策略"></a>消费者分区分配策略</h3><h4 id="Range范围分配策略"><a href="#Range范围分配策略" class="headerlink" title="Range范围分配策略"></a>Range范围分配策略</h4><p>Range范围分配策略是Kafka默认的分配策略，它可以确保每个消费者消费的分区数量是均衡的。</p>
<p>注意：Range范围分配策略是针对每个Topic的。</p>
<p><strong>配置</strong></p>
<p>配置消费者的<code>partition.assignment.strategy</code>为<code>org.apache.kafka.clients.consumer.RangeAssignor</code>。</p>
<p><strong>算法公式</strong></p>
<p>n = 分区数量 / 消费者数量</p>
<p>m = 分区数量 % 消费者数量</p>
<p>前m个消费者消费n+1个</p>
<p>剩余消费者消费n个</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305214111243.png" alt="image-20230305214111243"></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305214406647.png" alt="image-20230305214406647"></p>
<h4 id="RoundRobin轮询策略"><a href="#RoundRobin轮询策略" class="headerlink" title="RoundRobin轮询策略"></a>RoundRobin轮询策略</h4><p>RoundRobinAssignor轮询策略是将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序（topic和分区的hashcode进行排序），然后通过轮询方式逐个将分区以此分配给每个消费者。</p>
<p><strong>配置</strong></p>
<p>配置消费者的<code>partition.assignment.strategy</code>为<code>org.apache.kafka.clients.consumer.RoundRobinAssignor</code>。</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305214555487.png" alt="image-20230305214555487"></p>
<h4 id="Stricky粘性分配策略"><a href="#Stricky粘性分配策略" class="headerlink" title="Stricky粘性分配策略"></a>Stricky粘性分配策略</h4><p>从Kafka 0.11.x开始，引入此类分配策略。主要目的：</p>
<ol>
<li><p>分区分配尽可能均匀</p>
</li>
<li><p>在发生rebalance的时候，分区的分配尽可能与上一次分配保持相同</p>
</li>
</ol>
<p>没有发生rebalance时，Striky粘性分配策略和RoundRobin分配策略类似。</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305214816094.png" alt="image-20230305214816094"></p>
<p>上面如果consumer2崩溃了，此时需要进行rebalance。如果是Range分配和轮询分配都会重新进行分配，例如：</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305214922864.png" alt="image-20230305214922864"></p>
<p>通过上图，我们发现，consumer0和consumer1原来消费的分区大多发生了改变。接下来我们再来看下粘性分配策略。</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305214947583.png" alt="image-20230305214947583"></p>
<p>我们发现，Striky粘性分配策略，保留rebalance之前的分配结果。这样，只是将原先consumer2负责的两个分区再均匀分配给consumer0、consumer1。这样可以明显减少系统资源的浪费，例如：之前consumer0、consumer1之前正在消费某几个分区，但由于rebalance发生，导致consumer0、consumer1需要重新消费之前正在处理的分区，导致不必要的系统开销。（例如：某个事务正在进行就必须要取消了）</p>
<h3 id="副本机制"><a href="#副本机制" class="headerlink" title="副本机制"></a>副本机制</h3><p>副本的目的就是冗余备份，当某个Broker上的分区数据丢失时，依然可以保障数据可用。因为在其他的Broker上的副本是可用的。</p>
<h4 id="producer的ACKs参数"><a href="#producer的ACKs参数" class="headerlink" title="producer的ACKs参数"></a>producer的ACKs参数</h4><p>对副本关系较大的就是，producer配置的<code>acks</code>参数了,<code>acks</code>参数表示当生产者生产消息的时候，写入到副本的要求严格程度。它决定了生产者如何在性能和可靠性之间做取舍。</p>
<p>配置：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1.itcast.cn:9092&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br></pre></td></tr></table></figure>

<h4 id="acks配置为0"><a href="#acks配置为0" class="headerlink" title="acks配置为0"></a>acks配置为0</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305215548012.png" alt="image-20230305215548012"></p>
<p>ACK为0，基准测试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-producer-perf-test.sh --topic benchmark --num-records 5000000 --throughput -1 --record-size 1000 --producer-props bootstrap.servers=10.211.55.8:9092 acks=0</span><br></pre></td></tr></table></figure>

<p>测试结果：</p>
<table>
<thead>
<tr>
<th><strong>指标</strong></th>
<th><strong>单分区单副本（ack=0）</strong></th>
<th><strong>单分区单副本(ack=1)</strong></th>
</tr>
</thead>
<tbody><tr>
<td>吞吐量</td>
<td>47359.248314 records/sec  每秒4.7W条记录</td>
<td>40763.417279 records/sec   每秒4W条记录</td>
</tr>
<tr>
<td>吞吐速率</td>
<td>45.17 MB/sec  每秒约45MB数据</td>
<td>38.88 MB/sec  每秒约89MB数据</td>
</tr>
<tr>
<td>平均延迟时间</td>
<td>686.49 ms avg latency</td>
<td>799.67 ms avg latency</td>
</tr>
<tr>
<td>最大延迟时间</td>
<td>1444.00 ms max latency</td>
<td>1961.00 ms max latency</td>
</tr>
</tbody></table>
<h4 id="acks配置为1"><a href="#acks配置为1" class="headerlink" title="acks配置为1"></a>acks配置为1</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305220619546.png" alt="image-20230305220619546"></p>
<p>当生产者的ACK配置为1时，生产者会等待leader副本确认接收后，才会发送下一条数据，性能中等。</p>
<h4 id="acks配置为-1或者all"><a href="#acks配置为-1或者all" class="headerlink" title="acks配置为-1或者all"></a>acks配置为-1或者all</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230305220954110.png" alt="image-20230305220954110"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-producer-perf-test.sh --topic benchmark --num-records 5000000 --throughput -1 --record-size 1000 --producer-props bootstrap.servers=10.211.55.8:9092,10.211.55.9:9092,10.211.55.7:9092 acks=0</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th><strong>指标</strong></th>
<th><strong>单分区单副本（ack=0）</strong></th>
<th><strong>单分区单副本(ack=1)</strong></th>
<th><strong>单分区单副本(ack=-1/all)</strong></th>
</tr>
</thead>
<tbody><tr>
<td>吞吐量</td>
<td>47359.248314 records/sec  每秒4.7W条记录</td>
<td>40763.417279 records/sec   每秒4W条记录</td>
<td>540.5 /s  每秒7.3W调记录</td>
</tr>
<tr>
<td>吞吐速率</td>
<td>45.17 MB/sec  每秒约45MB数据</td>
<td>38.88 MB/sec  每秒约89MB数据</td>
<td>0.52 MB/sec</td>
</tr>
<tr>
<td>平均延迟时间</td>
<td>686.49 ms avg latency</td>
<td>799.67 ms avg latency</td>
<td>120281.8 ms</td>
</tr>
<tr>
<td>最大延迟时间</td>
<td>1444.00 ms max latency</td>
<td>1961.00 ms max latency</td>
<td>1884.00 ms</td>
</tr>
</tbody></table>
<h2 id="高级（High-Level）API与低级（Low-Level）API"><a href="#高级（High-Level）API与低级（Low-Level）API" class="headerlink" title="高级（High Level）API与低级（Low Level）API"></a>高级（High Level）API与低级（Low Level）API</h2><h3 id="高级API"><a href="#高级API" class="headerlink" title="高级API"></a>高级API</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 消费者程序：从test主题中消费数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">_2ConsumerTest</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1. 创建Kafka消费者配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;192.168.88.100:9092&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 创建Kafka消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 订阅要消费的主题</span></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">&quot;test&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4. 使用一个while循环，不断从Kafka的topic中拉取消息</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 定义100毫秒超时</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line">                System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, record.offset(), record.key(), record.value());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>上面是之前编写的代码，消费Kafka的消息很容易实现，写起来比较简单</li>
<li>不需要执行去管理<code>offset</code>，直接通过ZK管理；也不需要管理分区、副本，由Kafka统一管理</li>
<li>消费者会自动根据上一次在ZK中保存的<code>offset</code>去接着获取数据</li>
<li>在ZK中，不同的消费者组（group）同一个topic记录不同的<code>offset</code>，这样不同程序读取同一个topic，不会受<code>offset</code>的影响</li>
</ul>
<p><strong>高级API的缺点</strong></p>
<ul>
<li>不能控制offset，例如：想从指定的位置读取</li>
<li>不能细化控制分区、副本、ZK等</li>
</ul>
<h3 id="低级API"><a href="#低级API" class="headerlink" title="低级API"></a>低级API</h3><p>通过使用低级API，我们可以自己来控制offset，想从哪儿读，就可以从哪儿读。而且，可以自己控制连接分区，对分区自定义负载均衡。而且，之前offset是自动保存在ZK中，使用低级API，我们可以将offset不一定要使用ZK存储，我们可以自己来存储offset。例如：存储在文件、MySQL、或者内存中。但是低级API，比较复杂，需要执行控制offset，连接到哪个分区，并找到分区的leader。</p>
<h3 id="手动消费分区数据"><a href="#手动消费分区数据" class="headerlink" title="手动消费分区数据"></a>手动消费分区数据</h3><p>之前的代码，我们让Kafka根据消费组中的消费者动态地为topic分配要消费的分区。但在某些时候，我们需要指定要消费的分区，例如：</p>
<ul>
<li>如果某个程序将某个指定分区的数据保存到外部存储中，例如：Redis、MySQL，那么保存数据的时候，只需要消费该指定的分区数据即可</li>
<li>如果某个程序是高可用的，在程序出现故障时将自动重启(例如：后面我们将学习的Flink、Spark程序)。这种情况下，程序将从指定的分区重新开始消费数据。</li>
</ul>
<p>如何进行手动消费分区中的数据呢？</p>
<ol>
<li>不再使用之前的 subscribe 方法订阅主题，而使用 「assign」方法指定想要消费的消息</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> <span class="string">&quot;test&quot;</span>;</span><br><span class="line">    <span class="type">TopicPartition</span> <span class="variable">partition0</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(topic, <span class="number">0</span>);</span><br><span class="line">    <span class="type">TopicPartition</span> <span class="variable">partition1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(topic, <span class="number">1</span>);</span><br><span class="line">    consumer.assign(Arrays.asList(partition0, partition1));</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>一旦指定了分区，就可以就像前面的示例一样，在循环中调用「poll」方法消费消息</li>
</ol>
<p><strong>注意</strong></p>
<ol>
<li><p>当手动管理消费分区时，即使GroupID是一样的，Kafka的组协调器都将不再起作用</p>
</li>
<li><p>如果消费者失败，也将不再自动进行分区重新分配</p>
</li>
</ol>
<h2 id="监控工具Kafka-eagle介绍"><a href="#监控工具Kafka-eagle介绍" class="headerlink" title="监控工具Kafka-eagle介绍"></a>监控工具Kafka-eagle介绍</h2><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306112953617.png" alt="image-20230306112953617"></p>
<h3 id="Kafka-Eagle简介"><a href="#Kafka-Eagle简介" class="headerlink" title="Kafka-Eagle简介"></a>Kafka-Eagle简介</h3><p>在开发工作中，当业务前提不复杂时，可以使用Kafka命令来进行一些集群的管理工作。但如果业务变得复杂，例如：我们需要增加group、topic分区，此时，我们再使用命令行就感觉很不方便，此时，如果使用一个可视化的工具帮助我们完成日常的管理工作，将会大大提高对于Kafka集群管理的效率，而且我们使用工具来监控消费者在Kafka中消费情况。 </p>
<p>早期，要监控Kafka集群我们可以使用Kafka Monitor以及Kafka Manager，但随着我们对监控的功能要求、性能要求的提高，这些工具已经无法满足。</p>
<p>Kafka Eagle是一款结合了目前大数据Kafka监控工具的特点，重新研发的一块开源免费的Kafka集群优秀的监控工具。它可以非常方便的监控生产环境中的offset、lag变化、partition分布、owner等。</p>
<p>官网地址：<a target="_blank" rel="noopener" href="https://www.kafka-eagle.org/">https://www.kafka-eagle.org/</a></p>
<h3 id="安装Kafka-Eagle"><a href="#安装Kafka-Eagle" class="headerlink" title="安装Kafka-Eagle"></a>安装Kafka-Eagle</h3><h4 id="开启Kafka-JMX端口"><a href="#开启Kafka-JMX端口" class="headerlink" title="开启Kafka JMX端口"></a>开启Kafka JMX端口</h4><h5 id="JMX接口"><a href="#JMX接口" class="headerlink" title="JMX接口"></a>JMX接口</h5><p>JMX(Java Management Extensions)是一个为应用程序植入管理功能的框架。JMX是一套标准的代理和服务，实际上，用户可以在任何Java应用程序中使用这些代理和服务实现管理。很多的一些软件都提供了JMX接口，来实现一些管理、监控功能。</p>
<h5 id="开启Kafka-JMX"><a href="#开启Kafka-JMX" class="headerlink" title="开启Kafka JMX"></a>开启Kafka JMX</h5><p>在启动Kafka的脚本前，添加：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd $&#123;KAFKA_HOME&#125;</span><br><span class="line">export JMX_PORT=9988</span><br><span class="line">nohup bin/kafka-server-start.sh config/server.properties &amp;</span><br></pre></td></tr></table></figure>

<h4 id="安装Kafka-Eagle-1"><a href="#安装Kafka-Eagle-1" class="headerlink" title="安装Kafka-Eagle"></a>安装Kafka-Eagle</h4><p>需提前准备好mysql数据库并创建<code>ke</code>数据库</p>
<ol>
<li><p>安装JDK，并配置好JAVA_HOME</p>
</li>
<li><p>将kafka_eagle上传，并解压到 /export/server 目录中</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd cd /export/software/</span><br><span class="line">tar -xvzf kafka-eagle-bin-3.0.1.tar.gz -C ../server/</span><br><span class="line">cd /export/server/kafka-eagle-bin-3.0.1/ </span><br><span class="line">tar -xvzf efak-web-3.0.1-bin.tar.gz </span><br><span class="line">cd /export/server/kafka-eagle-bin-3.0.1/efak-web-3.0.1</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>配置 kafka_eagle 环境变量。</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line">export KE_HOME=/export/server/kafka-eagle-bin-1.4.6/kafka-eagle-web-1.4.6</span><br><span class="line">export PATH=$PATH:$KE_HOME/bin</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>配置 kafka_eagle。使用vi打开conf目录下的<code>system-config.properties</code></li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">vim conf/system-config.properties</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改第4行，配置kafka集群别名</span></span><br><span class="line">kafka.eagle.zk.cluster.alias=cluster1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改第5行，配置ZK集群地址</span></span><br><span class="line">cluster1.zk.list=node1.itcast.cn:2181,node2.itcast.cn:2181,node3.itcast.cn:2181</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">注释第6行</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">cluster2.zk.list=xdn10:2181,xdn11:2181,xdn12:2181</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改第32行，打开图标统计</span></span><br><span class="line">kafka.eagle.metrics.charts=true</span><br><span class="line">kafka.eagle.metrics.retain=30</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">注释第69行，取消sqlite数据库连接配置</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">kafka.eagle.driver=org.sqlite.JDBC</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">kafka.eagle.url=jdbc:sqlite:/hadoop/kafka-eagle/db/ke.db</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">kafka.eagle.username=root</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">kafka.eagle.password=www.kafka-eagle.org</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改第77行，开启mys</span></span><br><span class="line">kafka.eagle.driver=com.mysql.jdbc.Driver</span><br><span class="line">kafka.eagle.url=jdbc:mysql://10.211.55.8:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull</span><br><span class="line">kafka.eagle.username=root</span><br><span class="line">kafka.eagle.password=52809329</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>配置JAVA_HOME</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/kafka-eagle-bin-3.0.1/efak-web-3.0.1/bin</span><br><span class="line">vim ke.sh</span><br><span class="line"># 在第24行添加JAVA_HOME环境配置</span><br><span class="line">export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-arm64</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>修改Kafka eagle可执行权限</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/kafka-eagle-bin-3.0.1/efak-web-3.0.1/bin</span><br><span class="line">chmod +x ke.sh</span><br></pre></td></tr></table></figure>

<ol start="7">
<li>启动 kafka_eagle</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./ke.sh start</span><br></pre></td></tr></table></figure>

<ol start="8">
<li>访问Kafka eagle，默认用户为admin，密码为：123456</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://10.211.55.8:8048/ke</span><br></pre></td></tr></table></figure>

<h3 id="Kafka度量指标"><a href="#Kafka度量指标" class="headerlink" title="Kafka度量指标"></a>Kafka度量指标</h3><h4 id="topic-list"><a href="#topic-list" class="headerlink" title="topic list"></a>topic list</h4><p>点击Topic下的List菜单，就可以展示当前Kafka集群中的所有topic。</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306161949098.png" alt="image-20230306161949098"></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306162019519.png" alt="image-20230306162019519"></p>
<table>
<thead>
<tr>
<th>指标</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>Brokers Spread</td>
<td>broker使用率</td>
</tr>
<tr>
<td>Brokers Skew</td>
<td>分区是否倾斜</td>
</tr>
<tr>
<td>Brokers Leader Skew</td>
<td>leader partition是否存在倾斜</td>
</tr>
</tbody></table>
<h4 id="生产者消息总计"><a href="#生产者消息总计" class="headerlink" title="生产者消息总计"></a>生产者消息总计</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306162329012.png" alt="image-20230306162329012"></p>
<h2 id="Kafka原理"><a href="#Kafka原理" class="headerlink" title="Kafka原理"></a>Kafka原理</h2><h3 id="分区的leader与follower"><a href="#分区的leader与follower" class="headerlink" title="分区的leader与follower"></a>分区的leader与follower</h3><h4 id="Leader和Follower"><a href="#Leader和Follower" class="headerlink" title="Leader和Follower"></a>Leader和Follower</h4><p>在Kafka中，每个<code>topic</code>都可以配置多个分区以及多个副本。每个分区都有一个<code>leader</code>以及0个或者多个<code>follower</code>，在创建<code>topic</code>时，Kafka会将每个分区的<code>leader</code>均匀地分配在每个<code>broker</code>上。我们正常使用kafka是感觉不到<code>leader</code>、<code>follower</code>的存在的。但其实，所有的读写操作都是由<code>leader</code>处理，而所有的<code>follower</code>都复制<code>leader</code>的日志数据文件，如果<code>leader</code>出现故障时，<code>follower</code>就会被选举为<code>leader</code><strong>。</strong>所以，可以这样说：</p>
<ul>
<li>Kafka中的<code>leader</code>负责处理<code>读写操作</code>，而<code>follower</code>只负责<code>副本数据的同步</code></li>
<li>如果<code>leader</code>出现故障，其他<code>follower</code>会被重新选举为<code>leader</code></li>
<li><code>follower</code>像一个<code>consumer</code>一样，拉取<code>leader</code>对应分区的数据，并保存到日志数据文件中</li>
</ul>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306200630181.png" alt="image-20230306200630181"></p>
<h4 id="查看某个partition的leader"><a href="#查看某个partition的leader" class="headerlink" title="查看某个partition的leader"></a>查看某个partition的leader</h4><p>使用<code>Kafka-eagle</code>查看某个<code>topic</code>的<code>partition</code>的<code>leader</code>在哪个服务器中。为了方便观察，我们创建一个名为<code>test</code>的3个分区、3个副本的<code>topic</code>。</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306201257833.png" alt="image-20230306201257833"></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306201415040.png" alt="image-20230306201415040"></p>
<h4 id="AR、ISR、OSR"><a href="#AR、ISR、OSR" class="headerlink" title="AR、ISR、OSR"></a>AR、ISR、OSR</h4><p>在实际环境中，<code>leader</code>有可能会出现一些故障，所以Kafka一定会选举出新的<code>leader</code>。在讲解leader选举之前，我们先要明确几个概念。Kafka中，把follower可以按照不同状态分为三类——<code>AR</code>、<code>ISR</code>、<code>OSR</code>。</p>
<ul>
<li>分区的所有副本称为 <code>「AR」</code>（Assigned Replicas——已分配的副本）</li>
<li>所有与<code>leader</code>副本保持一定程度同步的副本（包括 leader 副本在内）组成 <code>「ISR」</code>（In-Sync Replicas——在同步中的副本）</li>
<li>由于<code>follower</code>副本同步滞后过多的副本（不包括 leader 副本）组成 <code>「OSR」</code>（Out-of-Sync Replias）</li>
<li><code>AR = ISR + OSR</code></li>
<li>正常情况下，所有的<code>follower</code>副本都应该与<code>leader</code>副本保持同步，即<code>AR = ISR</code>，<code>OSR</code>集合为空。</li>
</ul>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306202506289.png" alt="image-20230306202506289"></p>
<h4 id="查看分区的ISR"><a href="#查看分区的ISR" class="headerlink" title="查看分区的ISR"></a>查看分区的ISR</h4><ol>
<li>使用Kafka Eagle查看某个Topic的partition的ISR有哪几个节点。</li>
</ol>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306202600726.png" alt="image-20230306202600726"></p>
<ol>
<li>尝试关闭id为3的broker（杀掉该broker的进程），参看topic的ISR情况。</li>
</ol>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306202749974.png" alt="image-20230306202749974"></p>
<h4 id="Leader选举"><a href="#Leader选举" class="headerlink" title="Leader选举"></a>Leader选举</h4><p>leader对于消息的写入以及读取是非常关键的，此时有两个疑问：</p>
<ol>
<li><p>Kafka如何确定某个partition是leader、哪个partition是follower呢？</p>
</li>
<li><p>某个leader崩溃了，如何快速确定另外一个leader呢？因为Kafka的吞吐量很高、延迟很低，所以选举leader必须非常快</p>
</li>
</ol>
<h5 id="如果leader崩溃，Kafka会如何？"><a href="#如果leader崩溃，Kafka会如何？" class="headerlink" title="如果leader崩溃，Kafka会如何？"></a>如果leader崩溃，Kafka会如何？</h5><p>使用Kafka Eagle找到某个partition的leader，再找到leader所在的broker。在Linux中强制杀掉该Kafka的进程，然后观察leader的情况。</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306204137651.png" alt="image-20230306204137651"></p>
<p>杀死broker2后partition0重新选举broker3为leader</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306204406984.png" alt="image-20230306204406984"></p>
<h5 id="Controller介绍"><a href="#Controller介绍" class="headerlink" title="Controller介绍"></a>Controller介绍</h5><ul>
<li>Kafka启动时，会在所有的<code>broker</code>中选择一个<code>controller</code></li>
<li>前面<code>leader</code>和<code>follower</code>是针对<code>partition</code>，而<code>controller</code>是针对<code>broker</code>的</li>
<li>创建<code>topic</code>、或者<code>添加分区</code>、<code>修改副本数量</code>之类的管理任务都是由<code>controller</code>完成的</li>
<li>Kafka分区<code>leader</code>的选举，也是由<code>controller</code>决定的</li>
</ul>
<h5 id="Controller的选举"><a href="#Controller的选举" class="headerlink" title="Controller的选举"></a>Controller的选举</h5><ul>
<li>在Kafka集群启动的时候，每个<code>broker</code>都会尝试去<code>ZooKeeper</code>上注册成为<code>Controller</code>（ZK临时节点）</li>
<li>但只有一个竞争成功，其他的<code>broker</code>会注册该节点的<code>监视器</code></li>
<li>一点该临时节点状态发生变化，就可以进行相应的处理</li>
<li><code>Controller</code>也是高可用的，一旦某个<code>broker</code>崩溃，其他的<code>broker</code>会重新注册为<code>Controller</code></li>
</ul>
<h5 id="找到当前Kafka集群的controller"><a href="#找到当前Kafka集群的controller" class="headerlink" title="找到当前Kafka集群的controller"></a>找到当前Kafka集群的controller</h5><ol>
<li><p>点击Kafka Tools的「Tools」菜单，找到「ZooKeeper Brower…」</p>
</li>
<li><p>点击左侧树形结构的controller节点，就可以查看到哪个broker是controller了。</p>
</li>
</ol>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306205729450.png" alt="image-20230306205729450"></p>
<h5 id="测试controller选举"><a href="#测试controller选举" class="headerlink" title="测试controller选举"></a>测试controller选举</h5><p>通过kafka tools找到controller所在的broker对应的kafka进程，杀掉该进程，重新打开ZooKeeper brower，观察kafka是否能够选举出来新的Controller。</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306210201988.png" alt="image-20230306210201988"></p>
<h5 id="Controller选举partition-leader"><a href="#Controller选举partition-leader" class="headerlink" title="Controller选举partition leader"></a>Controller选举partition leader</h5><ul>
<li>所有Partition的leader选举都由controller决定</li>
<li>controller会将leader的改变直接通过RPC的方式通知需为此作出响应的Broker</li>
<li>controller读取到当前分区的ISR，只要有一个Replica还幸存，就选择其中一个作为leader否则，则任意选这个一个Replica作为leader</li>
<li>如果该partition的所有Replica都已经宕机，则新的leader为-1</li>
</ul>
<p>具体来说，当一个分区的 leader 副本失效时，follower 副本会发现并向其它 broker 节点发送请求，申请成为该分区的新 leader。同时，每个 broker 节点会周期性地向 controller 节点发送心跳请求，汇报自己当前的状态和可用性信息。controller 节点会根据这些信息，选择一个健康的、可用的 broker 节点作为该分区的新 leader。</p>
<p>在选举新 leader 的过程中，controller 节点会参考如下因素：</p>
<ol>
<li>副本状态：只有处于 ISR（in-sync replicas）列表中的 follower 副本才有资格成为新 leader，因为它们的数据已经与 leader 同步。</li>
<li>副本位置：controller 节点会选择与原 leader 副本相同或更靠前的位置作为新 leader 的位置，以确保最小化数据丢失。</li>
<li>副本健康状况：controller 节点会优先选择健康的、可用的 broker 节点作为新 leader，以确保高可用性和服务质量。</li>
</ol>
<p>总之，controller 节点会综合考虑多个因素，选出一个最适合成为新 leader 的 broker 节点，从而保障 Kafka 集群的高可用性和稳定性。</p>
<p><strong>为什么不能通过ZK的方式来选举partition的leader？</strong></p>
<ul>
<li>Kafka集群如果业务很多的情况下，会有很多的partition</li>
<li>假设某个broker宕机，就会出现很多的partiton都需要重新选举leader</li>
<li>如果使用zookeeper选举leader，会给zookeeper带来巨大的压力。所以，kafka中leader的选举不能使用ZK来实现</li>
</ul>
<h4 id="leader负载均衡"><a href="#leader负载均衡" class="headerlink" title="leader负载均衡"></a>leader负载均衡</h4><h5 id="Preferred-Replica"><a href="#Preferred-Replica" class="headerlink" title="Preferred Replica"></a>Preferred Replica</h5><ul>
<li>Kafka中引入了一个叫做<code>「preferred-replica」</code>的概念，意思就是：<code>优先的Replica</code></li>
<li>在ISR列表中，第一个replica就是preferred-replica</li>
<li>第一个分区存放的broker，肯定就是preferred-replica</li>
<li>执行以下脚本可以将preferred-replica设置为leader，均匀分配每个分区的leader。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-leader-election.sh --bootstrap-server node1.itcast.cn:9092 --topic 主题 --partition=1 --election-type preferred</span><br></pre></td></tr></table></figure>

<h5 id="确保leader在broker中负载均衡"><a href="#确保leader在broker中负载均衡" class="headerlink" title="确保leader在broker中负载均衡"></a>确保leader在broker中负载均衡</h5><p>杀掉test主题的某个broker，这样kafka会重新分配leader。等到Kafka重新分配leader之后，再次启动kafka进程。此时：观察test主题各个分区leader的分配情况。</p>
<p><strong><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306213503968.png" alt="image-20230306213503968"></strong></p>
<p>此时，会造成leader分配是不均匀的，所以可以执行以下脚本来重新分配leader:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-leader-election.sh --bootstrap-server 10.211.55.8:9092 --topic test --partition=1 --election-type preferred</span><br></pre></td></tr></table></figure>

<p>–partition：指定需要重新分配leader的partition编号</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu3:/export/server/kafka_2.12-2.4.1# bin/kafka-leader-election.sh --bootstrap-server 10.211.55.8:9092 --topic test --partition=1 --election-type preferred</span><br><span class="line">Successfully completed leader election (PREFERRED) for partitions test-1</span><br></pre></td></tr></table></figure>

<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306213706090.png" alt="image-20230306213706090"></p>
<h3 id="Kafka生产、消费数据工作流程"><a href="#Kafka生产、消费数据工作流程" class="headerlink" title="Kafka生产、消费数据工作流程"></a>Kafka生产、消费数据工作流程</h3><h4 id="Kafka数据写入流程"><a href="#Kafka数据写入流程" class="headerlink" title="Kafka数据写入流程"></a>Kafka数据写入流程</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306214611502.png" alt="image-20230306214611502"></p>
<ul>
<li>生产者先从 zookeeper 的 “/brokers/topics/主题名/partitions/分区名/state”节点找到该 partition 的leader</li>
</ul>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306214903945.png" alt="image-20230306214903945"></p>
<ul>
<li>生产者在ZK中找到该ID找到对应的broker</li>
</ul>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306215050562.png" alt="image-20230306215050562"></p>
<ul>
<li>broker进程上的leader将消息写入到本地log中</li>
<li>follower从leader上拉取消息，写入到本地log，并向leader发送ACK</li>
<li>leader接收到所有的ISR中的Replica的ACK后，并向生产者返回ACK。</li>
</ul>
<h4 id="Kafka数据消费流程"><a href="#Kafka数据消费流程" class="headerlink" title="Kafka数据消费流程"></a>Kafka数据消费流程</h4><h5 id="两种消费模式"><a href="#两种消费模式" class="headerlink" title="两种消费模式"></a>两种消费模式</h5><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306215312539.png" alt="image-20230306215312539"></p>
<ul>
<li>kafka采用拉取模型，由消费者自己记录消费状态，每个消费者互相独立地顺序拉取每个分区的消息</li>
<li>消费者可以按照任意的顺序消费消息。比如，消费者可以重置到旧的偏移量，重新处理之前已经消费过的消息；或者直接跳到最近的位置，从当前的时刻开始消费。</li>
</ul>
<h5 id="Kafka消费数据流程"><a href="#Kafka消费数据流程" class="headerlink" title="Kafka消费数据流程"></a>Kafka消费数据流程</h5><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306215711730.png" alt="image-20230306215711730"></p>
<ul>
<li>每个consumer都可以根据分配策略（默认RangeAssignor），获得要消费的分区</li>
<li>获取到consumer对应的offset（默认从ZK中获取上一次消费的offset）</li>
<li>找到该分区的leader，拉取数据</li>
<li>消费者提交offset</li>
</ul>
<h3 id="Kafka的数据存储形式"><a href="#Kafka的数据存储形式" class="headerlink" title="Kafka的数据存储形式"></a>Kafka的数据存储形式</h3><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306221446868.png" alt="image-20230306221446868"></p>
<ul>
<li>一个topic由多个分区组成</li>
<li>一个分区（partition）由多个segment（段）组成</li>
<li>一个segment（段）由多个文件组成（log、index、timeindex）</li>
</ul>
<h4 id="存储日志"><a href="#存储日志" class="headerlink" title="存储日志"></a>存储日志</h4><p>接下来，我们来看一下Kafka中的数据到底是如何在磁盘中存储的。</p>
<ul>
<li>Kafka中的数据是保存在 /export/server/kafka_2.12-2.4.1/data中</li>
<li>消息是保存在以：「主题名-分区ID」的文件夹中的</li>
<li>数据文件夹中包含以下内容：</li>
</ul>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230306221727670.png" alt="image-20230306221727670"></p>
<p>这些分别对应：</p>
<table>
<thead>
<tr>
<th>文件名</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>00000000000000000000.index</td>
<td>索引文件，根据offset查找数据就是通过该索引文件来操作的</td>
</tr>
<tr>
<td>00000000000000000000.log</td>
<td>日志数据文件</td>
</tr>
<tr>
<td>00000000000000000000.timeindex</td>
<td>时间索引</td>
</tr>
<tr>
<td>leader-epoch-checkpoint</td>
<td>持久化每个partition leader对应的LEO  （log end offset、日志文件中下一条待写入消息的offset）</td>
</tr>
</tbody></table>
<p>每个日志文件的文件名为起始偏移量，因为每个分区的起始偏移量是0，所以，分区的日志文件都以0000000000000000000.log开始</p>
<p>默认的每个日志文件最大为「log.segment.bytes =1024<em>1024</em>1024」1G</p>
<p>为了简化根据offset查找消息，Kafka日志文件名设计为开始的偏移量</p>
<h5 id="观察测试"><a href="#观察测试" class="headerlink" title="观察测试"></a>观察测试</h5><p>为了方便测试观察，新创建一个topic：「test_10m」，该topic每个日志数据文件最大为10M</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper 10.211.55.8 --topic test_10m --replication-factor 2 --partitions 3 --config segment.bytes=10485760</span><br></pre></td></tr></table></figure>

<p>使用之前的生产者程序往「test_10m」主题中生产数据，可以观察到如下：</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230307164926265.png" alt="image-20230307164926265"></p>
<h5 id="写入消息"><a href="#写入消息" class="headerlink" title="写入消息"></a>写入消息</h5><p>新的消息总是写入到最后的一个日志文件中</p>
<p>该文件如果到达指定的大小（默认为：1GB）时，将滚动到一个新的文件中</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230307165109262.png" alt="image-20230307165109262"></p>
<h5 id="读取消息"><a href="#读取消息" class="headerlink" title="读取消息"></a>读取消息</h5><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230307165236902.png" alt="image-20230307165236902"></p>
<p>根据「offset」首先需要找到存储数据的 segment 段（注意：offset指定分区的全局偏移量）</p>
<p>然后根据这个「全局分区offset」找到相对于文件的「segment段offset」</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230307165349810.png" alt="image-20230307165349810"></p>
<ul>
<li>最后再根据 「segment段offset」读取消息</li>
<li>为了提高查询效率，每个文件都会维护对应的范围内存，查找的时候就是使用简单的二分查找</li>
</ul>
<h5 id="删除消息"><a href="#删除消息" class="headerlink" title="删除消息"></a>删除消息</h5><ul>
<li>在Kafka中，消息是会被<strong>定期清理</strong>的。一次删除一个segment段的日志文件</li>
<li>Kafka的日志管理器，会根据Kafka的配置，来决定哪些文件可以被删除</li>
</ul>
<h3 id="消息不丢失机制"><a href="#消息不丢失机制" class="headerlink" title="消息不丢失机制"></a>消息不丢失机制</h3><h4 id="broker数据不丢失"><a href="#broker数据不丢失" class="headerlink" title="broker数据不丢失"></a>broker数据不丢失</h4><p>生产者通过分区的leader写入数据后，所有在ISR中follower都会从leader中复制数据，这样，可以确保即使leader崩溃了，其他的follower的数据仍然是可用的</p>
<h4 id="1-1-1-生产者数据不丢失"><a href="#1-1-1-生产者数据不丢失" class="headerlink" title="1.1.1   生产者数据不丢失"></a>1.1.1   生产者数据不丢失</h4><ul>
<li>生产者连接leader写入数据时，可以通过ACK机制来确保数据已经成功写入。ACK机制有三个可选配置</li>
</ul>
<ol>
<li>配置ACK响应要求为 -1 时 —— 表示所有的节点都收到数据(leader和follower都接</li>
</ol>
<p>收到数据）</p>
<ol start="2">
<li><p>配置ACK响应要求为 1 时 —— 表示leader收到数据</p>
</li>
<li><p>配置ACK影响要求为 0 时 —— 生产者只负责发送数据，不关心数据是否丢失（这种情</p>
</li>
</ol>
<p>况可能会产生数据丢失，但性能是最好的）</p>
<ul>
<li>生产者可以采用同步和异步两种方式发送数据</li>
</ul>
<ol>
<li>同步：发送一批数据给kafka后，等待kafka返回结果</li>
<li>异步：发送一批数据给kafka，只是提供一个回调函数。</li>
</ol>
<p>说明：如果broker迟迟不给ack，而buﬀer又满了，开发者可以设置是否直接清空buﬀer中的数据。</p>
<h4 id="消费者数据不丢失"><a href="#消费者数据不丢失" class="headerlink" title="消费者数据不丢失"></a>消费者数据不丢失</h4><p>在消费者消费数据的时候，只要每个消费者记录好oﬀset值即可，就能保证数据不丢失。</p>
<h3 id="数据积压"><a href="#数据积压" class="headerlink" title="数据积压"></a>数据积压</h3><p>Kafka消费者消费数据的速度是非常快的，但如果由于处理Kafka消息时，由于有一些外部IO、或者是产生网络拥堵，就会造成Kafka中的数据积压（或称为数据堆积）。如果数据一直积压，会导致数据出来的实时性受到较大影响。</p>
<h4 id="使用Kafka-Eagle查看数据积压情况"><a href="#使用Kafka-Eagle查看数据积压情况" class="headerlink" title="使用Kafka-Eagle查看数据积压情况"></a>使用Kafka-Eagle查看数据积压情况</h4><p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230307173313570.png" alt="image-20230307173313570"></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230307173543138.png" alt="image-20230307173543138"></p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230307173531368.png" alt="image-20230307173531368"></p>
<h4 id="解决数据积压问题"><a href="#解决数据积压问题" class="headerlink" title="解决数据积压问题"></a>解决数据积压问题</h4><p>当Kafka出现数据积压问题时，首先要找到数据积压的原因。以下是在企业中出现数据积压的几个类场景。</p>
<h5 id="数据写入MySQL失败"><a href="#数据写入MySQL失败" class="headerlink" title="数据写入MySQL失败"></a>数据写入MySQL失败</h5><p><strong>问题描述</strong></p>
<p>某日运维人员找到开发人员，说某个topic的一个分区发生数据积压，这个topic非常重要，而且开始有用户投诉。运维非常紧张，赶紧重启了这台机器。重启之后，还是无济于事。</p>
<p><strong>问题分析</strong></p>
<p>消费这个topic的代码比较简单，主要就是消费topic数据，然后进行判断在进行数据库操作。运维通过kafka-eagle找到积压的topic，发现该topic的某个分区积压了几十万条的消息。</p>
<p>最后，通过查看日志发现，由于数据写入到MySQL中报错，导致消费分区的offset一自没有提交，所以数据积压严重。 </p>
<h5 id="因为网络延迟消费失败"><a href="#因为网络延迟消费失败" class="headerlink" title="因为网络延迟消费失败"></a>因为网络延迟消费失败</h5><p><strong>问题描述</strong></p>
<p>基于Kafka开发的系统平稳运行了两个月，突然某天发现某个topic中的消息出现数据积压，大概有几万条消息没有被消费。</p>
<p><strong>问题分析</strong></p>
<p>通过查看应用程序日志发现，有大量的消费超时失败。后查明原因，因为当天网络抖动，通过查看Kafka的消费者超时配置为50ms，随后，将消费的时间修改为500ms后问题解决。</p>
<h2 id="Kafka中数据清理（Log-Deletion）"><a href="#Kafka中数据清理（Log-Deletion）" class="headerlink" title="Kafka中数据清理（Log Deletion）"></a>Kafka中数据清理（Log Deletion）</h2><p>Kafka的消息存储在磁盘中，为了控制磁盘占用空间，Kafka需要不断地对过去的一些消息进行清理工作。Kafka的每个分区都有很多的日志文件，这样也是为了方便进行日志的清理。在Kafka中，提供两种日志清理方式：</p>
<p><code>日志删除</code>（Log Deletion）：按照指定的策略<strong>直接删除</strong>不符合条件的日志。</p>
<p><code>日志压缩</code>（Log Compaction）：按照消息的key进行整合，有相同key的但有不同value值，只保留最后一个版本。</p>
<p>在Kafka的broker或topic配置中：</p>
<table>
<thead>
<tr>
<th>配置项</th>
<th>配置值</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>log.cleaner.enable</td>
<td>true（默认）</td>
<td>开启自动清理日志功能</td>
</tr>
<tr>
<td>log.cleanup.policy</td>
<td>delete（默认）</td>
<td>删除日志</td>
</tr>
<tr>
<td>log.cleanup.policy</td>
<td>compaction</td>
<td>压缩日志</td>
</tr>
<tr>
<td>log.cleanup.policy</td>
<td>delete,compact</td>
<td>同时支持删除、压缩</td>
</tr>
</tbody></table>
<h3 id="日志删除"><a href="#日志删除" class="headerlink" title="日志删除"></a>日志删除</h3><p>日志删除是以段（segment日志）为单位来进行定期清理的。</p>
<h4 id="定时日志删除任务"><a href="#定时日志删除任务" class="headerlink" title="定时日志删除任务"></a>定时日志删除任务</h4><p>Kafka日志管理器中会有一个专门的日志删除任务来定期检测和删除不符合保留条件的日志分段文件，这个周期可以通过broker端参数log.retention.check.interval.ms来配置，默认值为300,000，即5分钟。当前日志分段的保留策略有3种：</p>
<ol>
<li><p>基于时间的保留策略</p>
</li>
<li><p>基于日志大小的保留策略</p>
</li>
<li><p>基于日志起始偏移量的保留策略</p>
</li>
</ol>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230307194628338.png" alt="image-20230307194628338"></p>
<h4 id="基于时间的保留策略"><a href="#基于时间的保留策略" class="headerlink" title="基于时间的保留策略"></a>基于时间的保留策略</h4><p>以下三种配置可以指定如果Kafka中的消息超过指定的阈值，就会将日志进行自动清理：</p>
<ul>
<li>log.retention.hours</li>
<li>log.retention.minutes</li>
<li>log.retention.ms</li>
</ul>
<p>其中，优先级为 log.retention.ms &gt; log.retention.minutes &gt; log.retention.hours。默认情况，在broker中，配置如下：</p>
<p>log.retention.hours=168</p>
<p>也就是，默认日志的保留时间为168小时，相当于保留7天。</p>
<p>删除日志分段时:</p>
<ol>
<li><p>从日志文件对象中所维护日志分段的跳跃表中移除待删除的日志分段，以保证没有线程对这些日志分段进行读取操作</p>
</li>
<li><p>将日志分段文件添加上“.deleted”的后缀（也包括日志分段对应的索引文件）</p>
</li>
</ol>
<p>Kafka的后台定时任务会定期删除这些“.deleted”为后缀的文件，这个任务的延迟执行时间可以通过file.delete.delay.ms参数来设置，默认值为60000，即1分钟。</p>
<h5 id="设置topic-5秒删除一次"><a href="#设置topic-5秒删除一次" class="headerlink" title="设置topic 5秒删除一次"></a>设置topic 5秒删除一次</h5><ol>
<li>为了方便观察，设置段文件的大小为1M。</li>
</ol>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230307194939270.png" alt="image-20230307194939270"></p>
<ol start="2">
<li>设置topic的删除策略</li>
</ol>
<p>key: retention.ms</p>
<p>value: 5000</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230307195136561.png" alt="image-20230307195136561"></p>
<p>尝试往topic中添加一些数据，等待一会，观察日志的删除情况。我们发现，日志会定期被标记为删除，然后被删除。</p>
<h4 id="基于日志大小的保留策略"><a href="#基于日志大小的保留策略" class="headerlink" title="基于日志大小的保留策略"></a>基于日志大小的保留策略</h4><p>日志删除任务会检查当前日志的大小是否超过设定的阈值来寻找可删除的日志分段的文件集合。可以通过broker端参数 log.retention.bytes 来配置，默认值为-1，表示无穷大。如果超过该大小，会自动将超出部分删除。</p>
<p>注意:</p>
<p>log.retention.bytes 配置的是日志文件的总大小，而不是单个的日志分段的大小，一个日志文件包含多个日志分段。</p>
<h4 id="基于日志起始偏移量保留策略"><a href="#基于日志起始偏移量保留策略" class="headerlink" title="基于日志起始偏移量保留策略"></a>基于日志起始偏移量保留策略</h4><p>每个segment日志都有它的起始偏移量，如果起始偏移量小于 logStartOffset，那么这些日志文件将会标记为删除。</p>
<h3 id="日志压缩（Log-Compaction）"><a href="#日志压缩（Log-Compaction）" class="headerlink" title="日志压缩（Log Compaction）"></a>日志压缩（Log Compaction）</h3><p>Log Compaction是默认的日志删除之外的清理过时数据的方式。它会将相同的key对应的数据只保留一个版本。</p>
<p><img src="/img/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/image-20230307200241952.png" alt="image-20230307200241952"></p>
<ul>
<li>Log Compaction执行后，offset将不再连续，但依然可以查询Segment</li>
<li>Log Compaction执行前后，日志分段中的每条消息偏移量保持不变。Log Compaction会生成一个新的Segment文件</li>
<li>Log Compaction是针对key的，在使用的时候注意每个消息的key不为空</li>
<li>基于Log Compaction可以保留key的最新更新，可以基于Log Compaction来恢复消费者的最新状态</li>
</ul>
<h2 id="Kafka配额限速机制（Quotas）"><a href="#Kafka配额限速机制（Quotas）" class="headerlink" title="Kafka配额限速机制（Quotas）"></a>Kafka配额限速机制（Quotas）</h2><p>生产者和消费者以极高的速度生产/消费大量数据或产生请求，从而占用broker上的全部资源，造成网络IO饱和。有了配额（Quotas）就可以避免这些问题。Kafka支持配额管理，从而可以对Producer和Consumer的produce&amp;fetch操作进行流量限制，防止个别业务压爆服务器。</p>
<h3 id="限制producer端速率"><a href="#限制producer端速率" class="headerlink" title="限制producer端速率"></a>限制producer端速率</h3><p>为所有client id设置默认值，以下为所有producer程序设置其TPS不超过1MB/s，即1048576/s，命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-configs.sh --zookeeper node1.itcast.cn:2181 --alter --add-config &#x27;producer_byte_rate=1048576&#x27; --entity-type clients --entity-default</span><br></pre></td></tr></table></figure>

<p>运行基准测试，观察生产消息的速率</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-producer-perf-test.sh --topic test --num-records 500000 --throughput -1 --record-size 1000 --producer-props bootstrap.servers=node1.itcast.cn:9092,node2.itcast.cn:9092,node3.itcast.cn:9092 acks=1</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<p>50000 records sent, 1108.156028 records/sec (1.06 MB/sec)</p>
<h3 id="限制consumer端速率"><a href="#限制consumer端速率" class="headerlink" title="限制consumer端速率"></a>限制consumer端速率</h3><p>对consumer限速与producer类似，只不过参数名不一样。</p>
<p>为指定的topic进行限速，以下为所有consumer程序设置topic速率不超过1MB/s，即1048576/s。命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-configs.sh --zookeeper node1.itcast.cn:2181 --alter --add-config &#x27;consumer_byte_rate=1048576&#x27; --entity-type clients --entity-default</span><br></pre></td></tr></table></figure>

<p>运行基准测试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-perf-test.sh --broker-list node1.itcast.cn:9092,node2.itcast.cn:9092,node3.itcast.cn:9092 --topic test --fetch-size 1048576 --messages 500000</span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<p>MB.sec：1.0743</p>
<h3 id="取消Kafka的Quota配置"><a href="#取消Kafka的Quota配置" class="headerlink" title="取消Kafka的Quota配置"></a>取消Kafka的Quota配置</h3><p>使用以下命令，删除Kafka的Quota配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-configs.sh --zookeeper node1.itcast.cn:2181 --alter --delete-config &#x27;producer_byte_rate&#x27; --entity-type clients --entity-default</span><br><span class="line">bin/kafka-configs.sh --zookeeper node1.itcast.cn:2181 --alter --delete-config &#x27;consumer_byte_rate&#x27; --entity-type clients --entity-default</span><br></pre></td></tr></table></figure>

</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://weishao-996.github.io">Wei Shao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://weishao-996.github.io/2023/02/20/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/">https://weishao-996.github.io/2023/02/20/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Kafka/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://weishao-996.github.io" target="_blank">WeiBlog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98/">黑马程序员</a><a class="post-meta__tags" href="/tags/Kafka/">Kafka</a></div><div class="post_share"><div class="social-share" data-image="/img/bg/WechatIMG48.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/02/23/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Zookeeper/" title="黑马程序员-Zookeeper"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">黑马程序员-Zookeeper</div></div></a></div><div class="next-post pull-right"><a href="/2023/01/13/%E5%B0%9A%E7%A1%85%E8%B0%B7-Spring-Security/" title="尚硅谷-Spring-Security"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">尚硅谷-Spring-Security</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/11/17/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Docker/" title="黑马程序员-Docker"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-17</div><div class="title">黑马程序员-Docker</div></div></a></div><div><a href="/2022/11/21/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-RabbitMQ/" title="黑马程序员-RabbitMQ"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-21</div><div class="title">黑马程序员-RabbitMQ</div></div></a></div><div><a href="/2022/10/08/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Redis/" title="黑马程序员-Redis"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-08</div><div class="title">黑马程序员-Redis</div></div></a></div><div><a href="/2022/08/26/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-SpringBoot2/" title="黑马程序员-SpringBoot2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-26</div><div class="title">黑马程序员-SpringBoot2</div></div></a></div><div><a href="/2023/02/23/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-Zookeeper/" title="黑马程序员-Zookeeper"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-23</div><div class="title">黑马程序员-Zookeeper</div></div></a></div><div><a href="/2023/06/26/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-jQuery/" title="黑马程序员-jQuery"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-26</div><div class="title">黑马程序员-jQuery</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/bg/WechatIMG48.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Wei Shao</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">32</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">23</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/weishao-996"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/weishao-996" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2427340869@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">写什么代码，一拳把地球打爆！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">消息队列简介</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97"><span class="toc-number">1.1.1.</span> <span class="toc-text">什么是消息队列</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="toc-number">1.1.2.</span> <span class="toc-text">消息队列中间件</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%ABKafka%E5%91%A2"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">为什么叫Kafka呢</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.1.3.</span> <span class="toc-text">消息队列的应用场景</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%BC%82%E6%AD%A5%E5%A4%84%E7%90%86"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">异步处理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E8%A7%A3%E8%80%A6"><span class="toc-number">1.1.3.2.</span> <span class="toc-text">系统解耦</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B5%81%E9%87%8F%E5%89%8A%E5%B3%B0"><span class="toc-number">1.1.3.3.</span> <span class="toc-text">流量削峰</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E5%A4%84%E7%90%86%EF%BC%88%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A2%86%E5%9F%9F%E5%B8%B8%E8%A7%81%EF%BC%89"><span class="toc-number">1.1.3.4.</span> <span class="toc-text">日志处理（大数据领域常见）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E3%80%81%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.1.4.</span> <span class="toc-text">生产者、消费者模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.1.5.</span> <span class="toc-text">消息队列的两种模式</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%82%B9%E5%AF%B9%E7%82%B9%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.1.5.1.</span> <span class="toc-text">点对点模式</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.1.5.2.</span> <span class="toc-text">发布订阅模式</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E7%AE%80%E4%BB%8B"><span class="toc-number">1.2.</span> <span class="toc-text">Kafka简介</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFKafka"><span class="toc-number">1.2.1.</span> <span class="toc-text">什么是Kafka</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Kafka%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.2.2.</span> <span class="toc-text">Kafka的应用场景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Kafka%E8%AF%9E%E7%94%9F%E8%83%8C%E6%99%AF"><span class="toc-number">1.2.3.</span> <span class="toc-text">Kafka诞生背景</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="toc-number">1.3.</span> <span class="toc-text">Kafka的优势</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%93%AA%E4%BA%9B%E5%85%AC%E5%8F%B8%E5%9C%A8%E4%BD%BF%E7%94%A8Kafka"><span class="toc-number">1.4.</span> <span class="toc-text">哪些公司在使用Kafka</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E7%94%9F%E6%80%81%E5%9C%88%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.5.</span> <span class="toc-text">Kafka生态圈介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E7%89%88%E6%9C%AC"><span class="toc-number">1.6.</span> <span class="toc-text">Kafka版本</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="toc-number">2.</span> <span class="toc-text">环境搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%90%AD%E5%BB%BAKafka%E9%9B%86%E7%BE%A4"><span class="toc-number">2.1.</span> <span class="toc-text">搭建Kafka集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90"><span class="toc-number">2.2.</span> <span class="toc-text">目录结构分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E4%B8%80%E9%94%AE%E5%90%AF%E5%8A%A8-%E5%85%B3%E9%97%AD%E8%84%9A%E6%9C%AC"><span class="toc-number">2.3.</span> <span class="toc-text">Kafka一键启动&#x2F;关闭脚本</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C"><span class="toc-number">3.</span> <span class="toc-text">基础操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAtopic"><span class="toc-number">3.1.</span> <span class="toc-text">创建topic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E6%B6%88%E6%81%AF%E5%88%B0Kafka"><span class="toc-number">3.2.</span> <span class="toc-text">生产消息到Kafka</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8EKafka%E6%B6%88%E8%B4%B9%E6%B6%88%E6%81%AF"><span class="toc-number">3.3.</span> <span class="toc-text">从Kafka消费消息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Kafka-Tools%E6%93%8D%E4%BD%9CKafka"><span class="toc-number">3.4.</span> <span class="toc-text">使用Kafka Tools操作Kafka</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%9E%E6%8E%A5Kafka%E9%9B%86%E7%BE%A4"><span class="toc-number">3.4.1.</span> <span class="toc-text">连接Kafka集群</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAtopic-1"><span class="toc-number">3.4.2.</span> <span class="toc-text">创建topic</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95"><span class="toc-number">4.</span> <span class="toc-text">Kafka基准测试</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95"><span class="toc-number">4.1.</span> <span class="toc-text">基准测试</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E1%E4%B8%AA%E5%88%86%E5%8C%BA1%E4%B8%AA%E5%89%AF%E6%9C%AC%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95"><span class="toc-number">4.1.1.</span> <span class="toc-text">基于1个分区1个副本的基准测试</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAtopic-2"><span class="toc-number">4.1.1.1.</span> <span class="toc-text">创建topic</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E6%B6%88%E6%81%AF%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95"><span class="toc-number">4.1.1.2.</span> <span class="toc-text">生产消息基准测试</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E6%B6%88%E6%81%AF%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95"><span class="toc-number">4.1.1.3.</span> <span class="toc-text">消费消息基准测试</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E3%E4%B8%AA%E5%88%86%E5%8C%BA1%E4%B8%AA%E5%89%AF%E6%9C%AC%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95"><span class="toc-number">4.1.2.</span> <span class="toc-text">基于3个分区1个副本的基准测试</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAtopic-3"><span class="toc-number">4.1.2.1.</span> <span class="toc-text">创建topic</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E6%B6%88%E6%81%AF%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95-1"><span class="toc-number">4.1.2.2.</span> <span class="toc-text">生产消息基准测试</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E6%B6%88%E6%81%AF%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95-1"><span class="toc-number">4.1.2.3.</span> <span class="toc-text">消费消息基准测试</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E1%E4%B8%AA%E5%88%86%E5%8C%BA3%E4%B8%AA%E5%89%AF%E6%9C%AC%E7%9A%84%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95"><span class="toc-number">4.1.3.</span> <span class="toc-text">基于1个分区3个副本的基准测试</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAtopic-4"><span class="toc-number">4.1.3.1.</span> <span class="toc-text">创建topic</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E6%B6%88%E6%81%AF%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95-2"><span class="toc-number">4.1.3.2.</span> <span class="toc-text">生产消息基准测试</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E6%B6%88%E6%81%AF%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95-2"><span class="toc-number">4.1.3.3.</span> <span class="toc-text">消费消息基准测试</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Java%E7%BC%96%E7%A8%8B%E6%93%8D%E4%BD%9CKafka"><span class="toc-number">5.</span> <span class="toc-text">Java编程操作Kafka</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8C%E6%AD%A5%E7%94%9F%E4%BA%A7%E6%B6%88%E6%81%AF%E5%88%B0Kafka%E4%B8%AD"><span class="toc-number">5.1.</span> <span class="toc-text">同步生产消息到Kafka中</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9C%80%E6%B1%82"><span class="toc-number">5.1.1.</span> <span class="toc-text">需求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-number">5.1.2.</span> <span class="toc-text">准备工作</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AF%BC%E5%85%A5Maven-Kafka-POM%E4%BE%9D%E8%B5%96"><span class="toc-number">5.1.2.1.</span> <span class="toc-text">导入Maven Kafka POM依赖</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AF%BC%E5%85%A5log4j-properties"><span class="toc-number">5.1.2.2.</span> <span class="toc-text">导入log4j.properties</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%8C%85%E5%92%8C%E7%B1%BB"><span class="toc-number">5.1.2.3.</span> <span class="toc-text">创建包和类</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%BC%80%E5%8F%91"><span class="toc-number">5.1.3.</span> <span class="toc-text">代码开发</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8EKafka%E7%9A%84topic%E4%B8%AD%E6%B6%88%E8%B4%B9%E6%B6%88%E6%81%AF"><span class="toc-number">5.2.</span> <span class="toc-text">从Kafka的topic中消费消息</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9C%80%E6%B1%82-1"><span class="toc-number">5.2.1.</span> <span class="toc-text">需求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C-1"><span class="toc-number">5.2.2.</span> <span class="toc-text">准备工作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%80%E5%8F%91%E6%AD%A5%E9%AA%A4"><span class="toc-number">5.2.3.</span> <span class="toc-text">开发步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E4%BB%A3%E7%A0%81"><span class="toc-number">5.2.4.</span> <span class="toc-text">参考代码</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%82%E6%AD%A5%E4%BD%BF%E7%94%A8%E5%B8%A6%E6%9C%89%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0%E6%96%B9%E6%B3%95%E7%94%9F%E4%BA%A7%E6%B6%88%E6%81%AF"><span class="toc-number">5.3.</span> <span class="toc-text">异步使用带有回调函数方法生产消息</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84"><span class="toc-number">6.</span> <span class="toc-text">架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5"><span class="toc-number">6.1.</span> <span class="toc-text">Kafka重要概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#broker"><span class="toc-number">6.1.1.</span> <span class="toc-text">broker</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zookeeper"><span class="toc-number">6.1.2.</span> <span class="toc-text">zookeeper</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#producer%EF%BC%88%E7%94%9F%E4%BA%A7%E8%80%85%EF%BC%89"><span class="toc-number">6.1.3.</span> <span class="toc-text">producer（生产者）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#consumer%EF%BC%88%E6%B6%88%E8%B4%B9%E8%80%85%EF%BC%89"><span class="toc-number">6.1.4.</span> <span class="toc-text">consumer（消费者）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#consumer-group%EF%BC%88%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%EF%BC%89"><span class="toc-number">6.1.5.</span> <span class="toc-text">consumer group（消费者组）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%EF%BC%88Partitions%EF%BC%89"><span class="toc-number">6.1.6.</span> <span class="toc-text">分区（Partitions）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%AF%E6%9C%AC%EF%BC%88Replicas%EF%BC%89"><span class="toc-number">6.1.7.</span> <span class="toc-text">副本（Replicas）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BB%E9%A2%98%EF%BC%88Topic%EF%BC%89"><span class="toc-number">6.1.8.</span> <span class="toc-text">主题（Topic）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%81%8F%E7%A7%BB%E9%87%8F%EF%BC%88offset%EF%BC%89"><span class="toc-number">6.1.9.</span> <span class="toc-text">偏移量（offset）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84"><span class="toc-number">6.2.</span> <span class="toc-text">消费者组</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E7%94%9F%E4%BA%A7%E8%80%85%E5%B9%82%E7%AD%89%E6%80%A7%E4%B8%8E%E4%BA%8B%E5%8A%A1"><span class="toc-number">7.</span> <span class="toc-text">Kafka生产者幂等性与事务</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%82%E7%AD%89%E6%80%A7"><span class="toc-number">7.1.</span> <span class="toc-text">幂等性</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-1"><span class="toc-number">7.1.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Kafka%E7%94%9F%E4%BA%A7%E8%80%85%E5%B9%82%E7%AD%89%E6%80%A7"><span class="toc-number">7.1.2.</span> <span class="toc-text">Kafka生产者幂等性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E5%B9%82%E7%AD%89%E6%80%A7"><span class="toc-number">7.1.3.</span> <span class="toc-text">配置幂等性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%82%E7%AD%89%E6%80%A7%E5%8E%9F%E7%90%86"><span class="toc-number">7.1.4.</span> <span class="toc-text">幂等性原理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E4%BA%8B%E5%8A%A1"><span class="toc-number">7.2.</span> <span class="toc-text">Kafka事务</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-2"><span class="toc-number">7.2.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8B%E5%8A%A1%E6%93%8D%E4%BD%9CAPI"><span class="toc-number">7.2.2.</span> <span class="toc-text">事务操作API</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E4%BA%8B%E5%8A%A1%E7%BC%96%E7%A8%8B"><span class="toc-number">7.3.</span> <span class="toc-text">Kafka事务编程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8B%E5%8A%A1%E7%9B%B8%E5%85%B3%E5%B1%9E%E6%80%A7%E9%85%8D%E7%BD%AE"><span class="toc-number">7.3.1.</span> <span class="toc-text">事务相关属性配置</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85"><span class="toc-number">7.3.1.1.</span> <span class="toc-text">生产者</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85"><span class="toc-number">7.3.1.2.</span> <span class="toc-text">消费者</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Kafka%E4%BA%8B%E5%8A%A1%E7%BC%96%E7%A8%8B-1"><span class="toc-number">7.3.2.</span> <span class="toc-text">Kafka事务编程</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="toc-number">7.3.2.1.</span> <span class="toc-text">完整代码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%9C%80%E6%B1%82-2"><span class="toc-number">7.3.2.2.</span> <span class="toc-text">需求</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8%E7%94%9F%E4%BA%A7%E8%80%85%E6%8E%A7%E5%88%B6%E5%8F%B0%E7%A8%8B%E5%BA%8F%E6%A8%A1%E6%8B%9F%E6%95%B0%E6%8D%AE"><span class="toc-number">7.3.2.3.</span> <span class="toc-text">启动生产者控制台程序模拟数据</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BC%96%E5%86%99%E5%88%9B%E5%BB%BA%E6%B6%88%E8%B4%B9%E8%80%85%E4%BB%A3%E7%A0%81"><span class="toc-number">7.3.2.4.</span> <span class="toc-text">编写创建消费者代码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BC%96%E5%86%99%E5%88%9B%E5%BB%BA%E7%94%9F%E4%BA%A7%E8%80%85%E4%BB%A3%E7%A0%81"><span class="toc-number">7.3.2.5.</span> <span class="toc-text">编写创建生产者代码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81%E6%B6%88%E8%B4%B9%E5%B9%B6%E7%94%9F%E4%BA%A7%E6%95%B0%E6%8D%AE"><span class="toc-number">7.3.2.6.</span> <span class="toc-text">编写代码消费并生产数据</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-number">7.3.2.7.</span> <span class="toc-text">测试</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A8%A1%E6%8B%9F%E5%BC%82%E5%B8%B8%E6%B5%8B%E8%AF%95%E4%BA%8B%E5%8A%A1"><span class="toc-number">7.3.2.8.</span> <span class="toc-text">模拟异常测试事务</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E5%92%8C%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6"><span class="toc-number">8.</span> <span class="toc-text">分区和副本机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E5%88%86%E5%8C%BA%E5%86%99%E5%85%A5%E7%AD%96%E7%95%A5"><span class="toc-number">8.1.</span> <span class="toc-text">生产者分区写入策略</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BD%AE%E8%AF%A2%E7%AD%96%E7%95%A5"><span class="toc-number">8.1.1.</span> <span class="toc-text">轮询策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E7%AD%96%E7%95%A5%EF%BC%88%E4%B8%8D%E7%94%A8%EF%BC%89"><span class="toc-number">8.1.2.</span> <span class="toc-text">随机策略（不用）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8C%89key%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5"><span class="toc-number">8.1.3.</span> <span class="toc-text">按key分配策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B9%B1%E5%BA%8F%E9%97%AE%E9%A2%98"><span class="toc-number">8.1.4.</span> <span class="toc-text">乱序问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5"><span class="toc-number">8.1.5.</span> <span class="toc-text">自定义分区策略</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84Rebalance%E6%9C%BA%E5%88%B6"><span class="toc-number">8.2.</span> <span class="toc-text">消费者组Rebalance机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Rebalance%E5%86%8D%E5%9D%87%E8%A1%A1"><span class="toc-number">8.2.1.</span> <span class="toc-text">Rebalance再均衡</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Rebalance%E7%9A%84%E4%B8%8D%E8%89%AF%E5%BD%B1%E5%93%8D"><span class="toc-number">8.2.2.</span> <span class="toc-text">Rebalance的不良影响</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5"><span class="toc-number">8.3.</span> <span class="toc-text">消费者分区分配策略</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Range%E8%8C%83%E5%9B%B4%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5"><span class="toc-number">8.3.1.</span> <span class="toc-text">Range范围分配策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RoundRobin%E8%BD%AE%E8%AF%A2%E7%AD%96%E7%95%A5"><span class="toc-number">8.3.2.</span> <span class="toc-text">RoundRobin轮询策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Stricky%E7%B2%98%E6%80%A7%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5"><span class="toc-number">8.3.3.</span> <span class="toc-text">Stricky粘性分配策略</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6"><span class="toc-number">8.4.</span> <span class="toc-text">副本机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#producer%E7%9A%84ACKs%E5%8F%82%E6%95%B0"><span class="toc-number">8.4.1.</span> <span class="toc-text">producer的ACKs参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#acks%E9%85%8D%E7%BD%AE%E4%B8%BA0"><span class="toc-number">8.4.2.</span> <span class="toc-text">acks配置为0</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#acks%E9%85%8D%E7%BD%AE%E4%B8%BA1"><span class="toc-number">8.4.3.</span> <span class="toc-text">acks配置为1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#acks%E9%85%8D%E7%BD%AE%E4%B8%BA-1%E6%88%96%E8%80%85all"><span class="toc-number">8.4.4.</span> <span class="toc-text">acks配置为-1或者all</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7%EF%BC%88High-Level%EF%BC%89API%E4%B8%8E%E4%BD%8E%E7%BA%A7%EF%BC%88Low-Level%EF%BC%89API"><span class="toc-number">9.</span> <span class="toc-text">高级（High Level）API与低级（Low Level）API</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7API"><span class="toc-number">9.1.</span> <span class="toc-text">高级API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8E%E7%BA%A7API"><span class="toc-number">9.2.</span> <span class="toc-text">低级API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%B6%88%E8%B4%B9%E5%88%86%E5%8C%BA%E6%95%B0%E6%8D%AE"><span class="toc-number">9.3.</span> <span class="toc-text">手动消费分区数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7Kafka-eagle%E4%BB%8B%E7%BB%8D"><span class="toc-number">10.</span> <span class="toc-text">监控工具Kafka-eagle介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka-Eagle%E7%AE%80%E4%BB%8B"><span class="toc-number">10.1.</span> <span class="toc-text">Kafka-Eagle简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Kafka-Eagle"><span class="toc-number">10.2.</span> <span class="toc-text">安装Kafka-Eagle</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%80%E5%90%AFKafka-JMX%E7%AB%AF%E5%8F%A3"><span class="toc-number">10.2.1.</span> <span class="toc-text">开启Kafka JMX端口</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#JMX%E6%8E%A5%E5%8F%A3"><span class="toc-number">10.2.1.1.</span> <span class="toc-text">JMX接口</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%BC%80%E5%90%AFKafka-JMX"><span class="toc-number">10.2.1.2.</span> <span class="toc-text">开启Kafka JMX</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Kafka-Eagle-1"><span class="toc-number">10.2.2.</span> <span class="toc-text">安装Kafka-Eagle</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E5%BA%A6%E9%87%8F%E6%8C%87%E6%A0%87"><span class="toc-number">10.3.</span> <span class="toc-text">Kafka度量指标</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#topic-list"><span class="toc-number">10.3.1.</span> <span class="toc-text">topic list</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E6%81%AF%E6%80%BB%E8%AE%A1"><span class="toc-number">10.3.2.</span> <span class="toc-text">生产者消息总计</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E5%8E%9F%E7%90%86"><span class="toc-number">11.</span> <span class="toc-text">Kafka原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E7%9A%84leader%E4%B8%8Efollower"><span class="toc-number">11.1.</span> <span class="toc-text">分区的leader与follower</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Leader%E5%92%8CFollower"><span class="toc-number">11.1.1.</span> <span class="toc-text">Leader和Follower</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E6%9F%90%E4%B8%AApartition%E7%9A%84leader"><span class="toc-number">11.1.2.</span> <span class="toc-text">查看某个partition的leader</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#AR%E3%80%81ISR%E3%80%81OSR"><span class="toc-number">11.1.3.</span> <span class="toc-text">AR、ISR、OSR</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E5%88%86%E5%8C%BA%E7%9A%84ISR"><span class="toc-number">11.1.4.</span> <span class="toc-text">查看分区的ISR</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Leader%E9%80%89%E4%B8%BE"><span class="toc-number">11.1.5.</span> <span class="toc-text">Leader选举</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E6%9E%9Cleader%E5%B4%A9%E6%BA%83%EF%BC%8CKafka%E4%BC%9A%E5%A6%82%E4%BD%95%EF%BC%9F"><span class="toc-number">11.1.5.1.</span> <span class="toc-text">如果leader崩溃，Kafka会如何？</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Controller%E4%BB%8B%E7%BB%8D"><span class="toc-number">11.1.5.2.</span> <span class="toc-text">Controller介绍</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Controller%E7%9A%84%E9%80%89%E4%B8%BE"><span class="toc-number">11.1.5.3.</span> <span class="toc-text">Controller的选举</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%89%BE%E5%88%B0%E5%BD%93%E5%89%8DKafka%E9%9B%86%E7%BE%A4%E7%9A%84controller"><span class="toc-number">11.1.5.4.</span> <span class="toc-text">找到当前Kafka集群的controller</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95controller%E9%80%89%E4%B8%BE"><span class="toc-number">11.1.5.5.</span> <span class="toc-text">测试controller选举</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Controller%E9%80%89%E4%B8%BEpartition-leader"><span class="toc-number">11.1.5.6.</span> <span class="toc-text">Controller选举partition leader</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#leader%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1"><span class="toc-number">11.1.6.</span> <span class="toc-text">leader负载均衡</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Preferred-Replica"><span class="toc-number">11.1.6.1.</span> <span class="toc-text">Preferred Replica</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A1%AE%E4%BF%9Dleader%E5%9C%A8broker%E4%B8%AD%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1"><span class="toc-number">11.1.6.2.</span> <span class="toc-text">确保leader在broker中负载均衡</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E7%94%9F%E4%BA%A7%E3%80%81%E6%B6%88%E8%B4%B9%E6%95%B0%E6%8D%AE%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">11.2.</span> <span class="toc-text">Kafka生产、消费数据工作流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Kafka%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5%E6%B5%81%E7%A8%8B"><span class="toc-number">11.2.1.</span> <span class="toc-text">Kafka数据写入流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Kafka%E6%95%B0%E6%8D%AE%E6%B6%88%E8%B4%B9%E6%B5%81%E7%A8%8B"><span class="toc-number">11.2.2.</span> <span class="toc-text">Kafka数据消费流程</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%A4%E7%A7%8D%E6%B6%88%E8%B4%B9%E6%A8%A1%E5%BC%8F"><span class="toc-number">11.2.2.1.</span> <span class="toc-text">两种消费模式</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Kafka%E6%B6%88%E8%B4%B9%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B"><span class="toc-number">11.2.2.2.</span> <span class="toc-text">Kafka消费数据流程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E5%BD%A2%E5%BC%8F"><span class="toc-number">11.3.</span> <span class="toc-text">Kafka的数据存储形式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AD%98%E5%82%A8%E6%97%A5%E5%BF%97"><span class="toc-number">11.3.1.</span> <span class="toc-text">存储日志</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%A7%82%E5%AF%9F%E6%B5%8B%E8%AF%95"><span class="toc-number">11.3.1.1.</span> <span class="toc-text">观察测试</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%86%99%E5%85%A5%E6%B6%88%E6%81%AF"><span class="toc-number">11.3.1.2.</span> <span class="toc-text">写入消息</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%B6%88%E6%81%AF"><span class="toc-number">11.3.1.3.</span> <span class="toc-text">读取消息</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%A0%E9%99%A4%E6%B6%88%E6%81%AF"><span class="toc-number">11.3.1.4.</span> <span class="toc-text">删除消息</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E4%B8%8D%E4%B8%A2%E5%A4%B1%E6%9C%BA%E5%88%B6"><span class="toc-number">11.4.</span> <span class="toc-text">消息不丢失机制</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#broker%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E5%A4%B1"><span class="toc-number">11.4.1.</span> <span class="toc-text">broker数据不丢失</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-1-%E7%94%9F%E4%BA%A7%E8%80%85%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E5%A4%B1"><span class="toc-number">11.4.2.</span> <span class="toc-text">1.1.1   生产者数据不丢失</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E5%A4%B1"><span class="toc-number">11.4.3.</span> <span class="toc-text">消费者数据不丢失</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B"><span class="toc-number">11.5.</span> <span class="toc-text">数据积压</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Kafka-Eagle%E6%9F%A5%E7%9C%8B%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B%E6%83%85%E5%86%B5"><span class="toc-number">11.5.1.</span> <span class="toc-text">使用Kafka-Eagle查看数据积压情况</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B%E9%97%AE%E9%A2%98"><span class="toc-number">11.5.2.</span> <span class="toc-text">解决数据积压问题</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5MySQL%E5%A4%B1%E8%B4%A5"><span class="toc-number">11.5.2.1.</span> <span class="toc-text">数据写入MySQL失败</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9B%A0%E4%B8%BA%E7%BD%91%E7%BB%9C%E5%BB%B6%E8%BF%9F%E6%B6%88%E8%B4%B9%E5%A4%B1%E8%B4%A5"><span class="toc-number">11.5.2.2.</span> <span class="toc-text">因为网络延迟消费失败</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E4%B8%AD%E6%95%B0%E6%8D%AE%E6%B8%85%E7%90%86%EF%BC%88Log-Deletion%EF%BC%89"><span class="toc-number">12.</span> <span class="toc-text">Kafka中数据清理（Log Deletion）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E5%88%A0%E9%99%A4"><span class="toc-number">12.1.</span> <span class="toc-text">日志删除</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E6%97%B6%E6%97%A5%E5%BF%97%E5%88%A0%E9%99%A4%E4%BB%BB%E5%8A%A1"><span class="toc-number">12.1.1.</span> <span class="toc-text">定时日志删除任务</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%9A%84%E4%BF%9D%E7%95%99%E7%AD%96%E7%95%A5"><span class="toc-number">12.1.2.</span> <span class="toc-text">基于时间的保留策略</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AEtopic-5%E7%A7%92%E5%88%A0%E9%99%A4%E4%B8%80%E6%AC%A1"><span class="toc-number">12.1.2.1.</span> <span class="toc-text">设置topic 5秒删除一次</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%97%A5%E5%BF%97%E5%A4%A7%E5%B0%8F%E7%9A%84%E4%BF%9D%E7%95%99%E7%AD%96%E7%95%A5"><span class="toc-number">12.1.3.</span> <span class="toc-text">基于日志大小的保留策略</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%97%A5%E5%BF%97%E8%B5%B7%E5%A7%8B%E5%81%8F%E7%A7%BB%E9%87%8F%E4%BF%9D%E7%95%99%E7%AD%96%E7%95%A5"><span class="toc-number">12.1.4.</span> <span class="toc-text">基于日志起始偏移量保留策略</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E5%8E%8B%E7%BC%A9%EF%BC%88Log-Compaction%EF%BC%89"><span class="toc-number">12.2.</span> <span class="toc-text">日志压缩（Log Compaction）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E9%85%8D%E9%A2%9D%E9%99%90%E9%80%9F%E6%9C%BA%E5%88%B6%EF%BC%88Quotas%EF%BC%89"><span class="toc-number">13.</span> <span class="toc-text">Kafka配额限速机制（Quotas）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%90%E5%88%B6producer%E7%AB%AF%E9%80%9F%E7%8E%87"><span class="toc-number">13.1.</span> <span class="toc-text">限制producer端速率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%90%E5%88%B6consumer%E7%AB%AF%E9%80%9F%E7%8E%87"><span class="toc-number">13.2.</span> <span class="toc-text">限制consumer端速率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%96%E6%B6%88Kafka%E7%9A%84Quota%E9%85%8D%E7%BD%AE"><span class="toc-number">13.3.</span> <span class="toc-text">取消Kafka的Quota配置</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/06/28/PostgreSQL%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/" title="PostgreSQL基本使用">PostgreSQL基本使用</a><time datetime="2023-06-28T02:08:24.000Z" title="发表于 2023-06-28 10:08:24">2023-06-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/06/26/CentOS7%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85PostgreSQL12/" title="CentOS7离线安装PostgreSQL12">CentOS7离线安装PostgreSQL12</a><time datetime="2023-06-26T03:20:12.000Z" title="发表于 2023-06-26 11:20:12">2023-06-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/06/26/%E9%BB%91%E9%A9%AC%E7%A8%8B%E5%BA%8F%E5%91%98-jQuery/" title="黑马程序员-jQuery">黑马程序员-jQuery</a><time datetime="2023-06-25T16:00:37.000Z" title="发表于 2023-06-26 00:00:37">2023-06-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/06/16/%E5%90%89%E4%BB%96%E8%B0%B1-%E6%81%B6%E9%AD%94%E4%B9%8B%E5%AD%90/" title="吉他谱-恶魔之子">吉他谱-恶魔之子</a><time datetime="2023-06-16T06:24:13.000Z" title="发表于 2023-06-16 14:24:13">2023-06-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/22/%E4%B8%89%E6%9B%B4%E8%8D%89%E5%A0%82-SpringMVC/" title="三更草堂-SpringMVC">三更草堂-SpringMVC</a><time datetime="2023-05-22T07:24:57.000Z" title="发表于 2023-05-22 15:24:57">2023-05-22</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/bg/WechatIMG88.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Wei Shao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script>(function(){window.hypothesisConfig=function(){return{showHighlights:true,appType:'bookmarklet'};};var d=document,s=d.createElement('script');s.setAttribute('src','https://hypothes.is/embed.js');d.body.appendChild(s)})();</script></body></html>